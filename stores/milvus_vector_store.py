"""
Module triá»ƒn khai Milvus vector store
"""
import os
import uuid
from typing import Any, Dict, List, Optional, Union, Tuple
from pymilvus import Collection, connections, utility, FieldSchema, CollectionSchema
from pymilvus import DataType, MilvusException
from util.env_loader import get_env

from stores.base_vector_store import BaseVectorStore

class MilvusVectorStore(BaseVectorStore):
    """
    Triá»ƒn khai vector store sá»­ dá»¥ng Milvus
    """
    
    def __init__(
        self,
        collection_name: str,
        embedding_function: Any,
        uri: Optional[str] = None,
        db_name: Optional[str] = None,
        user: Optional[str] = None,
        password: Optional[str] = None,
        text_field: str = "text", # Field for text content
        # vector_field is removed, specific fields below
        id_field: str = "id",
        metadata_fields: Optional[List[Tuple[str, str]]] = None, # e.g., [("metadata", DataType.JSON)]
        clip_dimension: int = 512, # Default CLIP dimension
        resnet_dimension: int = 2048, # Default ResNet dimension
        text_vector_field: str = "vector", # Field name for text embeddings
        recreate_collection: bool = False,
        **kwargs
    ):
        """
        Khá»Ÿi táº¡o Milvus vector store
        
        Args:
            collection_name: TÃªn cá»§a collection
            embedding_function: HÃ m embedding (primarily for text). Can be None if only adding precomputed vectors.
            uri: URI káº¿t ná»‘i tá»›i Milvus server. Máº·c Ä‘á»‹nh láº¥y tá»« biáº¿n mÃ´i trÆ°á»ng MILVUS_URI
            db_name: TÃªn database. Máº·c Ä‘á»‹nh láº¥y tá»« biáº¿n mÃ´i trÆ°á»ng MILVUS_DB_NAME
            user: TÃªn ngÆ°á»i dÃ¹ng. Máº·c Ä‘á»‹nh láº¥y tá»« biáº¿n mÃ´i trÆ°á»ng MILVUS_USER
            password: Máº­t kháº©u. Máº·c Ä‘á»‹nh láº¥y tá»« biáº¿n mÃ´i trÆ°á»ng MILVUS_PASSWORD
            text_field: TÃªn trÆ°á»ng chá»©a vÄƒn báº£n gá»‘c.
            id_field: TÃªn trÆ°á»ng chá»©a ID chÃ­nh (primary key).
            metadata_fields: Danh sÃ¡ch cÃ¡c trÆ°á»ng metadata bá»• sung vÃ  kiá»ƒu dá»¯ liá»‡u (e.g., [("metadata", DataType.JSON)]).
            clip_dimension: Sá»‘ chiá»u cho vector CLIP.
            resnet_dimension: Sá»‘ chiá»u cho vector ResNet.
            text_vector_field: TÃªn trÆ°á»ng Ä‘á»ƒ lÆ°u vector embedding cá»§a text.
            recreate_collection: CÃ³ táº¡o láº¡i collection náº¿u Ä‘Ã£ tá»“n táº¡i khÃ´ng
        """
        self.collection_name = collection_name
        self.embedding_function = embedding_function # Used for text embedding and potentially dim inference
        self.text_field = text_field
        self.id_field = id_field
        self.clip_dimension = clip_dimension
        self.resnet_dimension = resnet_dimension
        self.text_vector_field = text_vector_field
        
        # Láº¥y thÃ´ng tin káº¿t ná»‘i tá»« biáº¿n mÃ´i trÆ°á»ng náº¿u khÃ´ng Ä‘Æ°á»£c cung cáº¥p
        self.uri = uri or get_env("MILVUS_URI", "http://localhost:19530")
        self.db_name = db_name or get_env("MILVUS_DB_NAME", "default")
        print(f"MILVUS_DB_NAME: {self.db_name}")
        self.user = user or get_env("MILVUS_USER", "")
        self.password = password or get_env("MILVUS_PASSWORD", "")
        
        # Äá»‹nh nghÄ©a cÃ¡c trÆ°á»ng metadata
        self.metadata_fields = metadata_fields or [
            ("metadata", DataType.JSON)  # Máº·c Ä‘á»‹nh cÃ³ má»™t trÆ°á»ng metadata dáº¡ng JSON
        ]
        
        try:
            # Káº¿t ná»‘i tá»›i Milvus server
            self._connect()
            
            # Kiá»ƒm tra vÃ  táº¡o collection náº¿u cáº§n
            self._init_collection(recreate_collection)
        except MilvusException as e:
            print(f"Lá»—i káº¿t ná»‘i Milvus: {e}")
            print(f"Kiá»ƒm tra láº¡i cáº¥u hÃ¬nh káº¿t ná»‘i:")
            print(f"- URI: {self.uri}")
            print(f"- Database: {self.db_name}")
            print(f"- User: {self.user}")
            raise
    
    def _connect(self) -> None:
        """
        Káº¿t ná»‘i tá»›i Milvus server
        """
        try:
            # Káº¿t ná»‘i tá»›i Milvus server vá»›i database Ä‘Ã£ chá»n
            connections.connect(
                alias="default", 
                uri=self.uri,
                db_name=self.db_name,
                user=self.user,
                password=self.password
            )
            print(f"ÄÃ£ káº¿t ná»‘i tá»›i database '{self.db_name}'")
        except Exception as e:
            # Náº¿u lá»—i lÃ  do database khÃ´ng tá»“n táº¡i, thá»­ táº¡o má»›i
            if "database not found" in str(e).lower():
                # Káº¿t ná»‘i láº¡i khÃ´ng chá»‰ Ä‘á»‹nh database
                connections.connect(
                    alias="default", 
                    uri=self.uri,
                    user=self.user,
                    password=self.password
                )
                # Táº¡o database má»›i
                from pymilvus import db
                db.create_database(self.db_name)
                print(f"ÄÃ£ táº¡o database '{self.db_name}'")
                
                # Káº¿t ná»‘i láº¡i vá»›i database má»›i
                connections.disconnect("default")
                connections.connect(
                    alias="default", 
                    uri=self.uri,
                    db_name=self.db_name,
                    user=self.user,
                    password=self.password
                )
                print(f"ÄÃ£ káº¿t ná»‘i tá»›i database '{self.db_name}'")
            else:
                print(f"Lá»—i khi káº¿t ná»‘i: {e}")
                raise
    
    def _get_vector_dim(self) -> int:
        """
        Láº¥y sá»‘ chiá»u cá»§a vector embedding
        
        Returns:
            int: Sá»‘ chiá»u cá»§a vector hoáº·c -1 náº¿u khÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh.
        """
        if self.embedding_function:
            try:
                # Táº¡o má»™t vector tá»« vÄƒn báº£n máº«u Ä‘á»ƒ xÃ¡c Ä‘á»‹nh sá»‘ chiá»u
                sample_vector = self.embedding_function("Sample text")
                if isinstance(sample_vector, list):
                    return len(sample_vector)
                elif hasattr(sample_vector, 'shape'):
                     return sample_vector.shape[0]
            except Exception as e:
                print(f"Warning: KhÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh sá»‘ chiá»u tá»« embedding_function: {e}")
        # Tráº£ vá» giÃ¡ trá»‹ máº·c Ä‘á»‹nh hoáº·c bÃ¡o lá»—i náº¿u khÃ´ng cÃ³ embedding_function
        # Hoáº·c dá»±a vÃ o collection schema náº¿u Ä‘Ã£ tá»“n táº¡i? Táº¡m thá»i tráº£ vá» -1
        print("Warning: KhÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh sá»‘ chiá»u vector tá»« embedding_function.")
        return -1 # Hoáº·c má»™t giÃ¡ trá»‹ máº·c Ä‘á»‹nh khÃ¡c / raise error
    
    def _create_optimized_index(self, field_name: str = "vector") -> None:
        """
        Táº¡o index tá»‘i Æ°u cho vector field - SINGLE SOURCE OF TRUTH
        
        Args:
            field_name: TÃªn field cáº§n Ä‘Ã¡nh index (máº·c Ä‘á»‹nh "vector")
        """
        # Tham sá»‘ index tá»‘i Æ°u duy nháº¥t - KHÃ”NG duplicate ná»¯a
        index_params = {
            "metric_type": "COSINE", 
            "index_type": "HNSW", 
            "params": {
                "M": 16,             # Tá»‘i Æ°u cho cháº¥t lÆ°á»£ng vÃ  hiá»‡u suáº¥t
                "efConstruction": 200 # Tá»‘i Æ°u Ä‘á»ƒ giáº£m phÃ¢n máº£nh
            }
        }
        
        try:
            self.collection.create_index(field_name, index_params)
            print(f"âœ… ÄÃ£ táº¡o index tá»‘i Æ°u cho field '{field_name}' (M=16, efConstruction=200)")
        except Exception as e:
            print(f"âŒ Lá»—i khi táº¡o index cho '{field_name}': {e}")
            raise
    
    def _batch_embed_texts(self, texts: List[str], embedding_batch_size: int = 200) -> List[List[float]]:
        """
        Batch embedding vá»›i sub-batches Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i API
        
        Args:
            texts: Danh sÃ¡ch texts cáº§n embed
            embedding_batch_size: KÃ­ch thÆ°á»›c sub-batch (máº·c Ä‘á»‹nh 200)
            
        Returns:
            List[List[float]]: Danh sÃ¡ch vectors
        """
        import time  # Import time module
        
        if not texts:
            return []
            
        if len(texts) == 1:
            # Single text
            return [self.embedding_function(texts[0])]
        
        # Batch embedding vá»›i sub-batches
        all_vectors = []
        total_sub_batches = (len(texts) + embedding_batch_size - 1) // embedding_batch_size
        
        print(f"      ğŸ§  Chia thÃ nh {total_sub_batches} sub-batches ({embedding_batch_size} records/batch)...")
        
        for sub_i in range(0, len(texts), embedding_batch_size):
            sub_batch_texts = texts[sub_i:sub_i + embedding_batch_size]
            sub_batch_num = (sub_i // embedding_batch_size) + 1
            
            sub_start = time.time()
            print(f"         ğŸ”„ Sub-batch {sub_batch_num}/{total_sub_batches}: {len(sub_batch_texts)} texts...")
            
            # Gá»i embedding cho sub-batch
            sub_vectors = self.embedding_function(sub_batch_texts)
            all_vectors.extend(sub_vectors)
            
            sub_time = time.time() - sub_start
            sub_progress = (sub_batch_num / total_sub_batches) * 100
            sub_speed = len(sub_batch_texts) / sub_time if sub_time > 0 else 0
            
            print(f"         âœ… Sub-batch {sub_batch_num} hoÃ n thÃ nh ({sub_progress:.0f}%) - {sub_time:.1f}s - {sub_speed:.0f} texts/s")
        
        return all_vectors
    
    def _init_inventory_item_collection(self) -> None:
        """
        Khá»Ÿi táº¡o collection inventory_item
        """
        # XÃ¡c Ä‘á»‹nh sá»‘ chiá»u cá»§a vector
        dim = self._get_vector_dim()
        
        # Äá»‹nh nghÄ©a schema cho collection
        fields = [
            FieldSchema(name=self.id_field, dtype=DataType.VARCHAR, is_primary=True, max_length=100),
            FieldSchema(name=self.text_field, dtype=DataType.VARCHAR, max_length=65535),
            FieldSchema(name=self.text_vector_field, dtype=DataType.FLOAT_VECTOR, dim=dim),
            FieldSchema(name="inventory_item_id", dtype=DataType.VARCHAR, max_length=100),
        ]
        
        # ThÃªm cÃ¡c trÆ°á»ng metadata
        for field_name, field_type in self.metadata_fields:
            fields.append(FieldSchema(name=field_name, dtype=field_type))
        
        # Táº¡o schema vÃ  collection
        try:
            schema = CollectionSchema(fields, enable_dynamic_field=False) # KhÃ´ng báº­t dynamic field trá»« khi thá»±c sá»± cáº§n
            self.collection = Collection(self.collection_name, schema)
            print(f"ÄÃ£ táº¡o collection '{self.collection_name}' vá»›i schema.")
        except Exception as e:
            print(f"Lá»—i khi táº¡o collection '{self.collection_name}': {e}")
            raise # NÃ©m láº¡i lá»—i Ä‘á»ƒ dá»«ng quÃ¡ trÃ¬nh náº¿u khÃ´ng táº¡o Ä‘Æ°á»£c collection

        # Sá»¬ Dá»¤NG METHOD DUY NHáº¤T Ä‘á»ƒ táº¡o index
        self._create_optimized_index("vector")

        # Load collection vÃ o memory sau khi táº¡o index
        print(f"Äang load collection '{self.collection_name}'...")
        self.collection.load()
        print(f"Collection '{self.collection_name}' Ä‘Ã£ Ä‘Æ°á»£c load.")
    
    def _init_health_check_collection(self) -> None:
        """
        Khá»Ÿi táº¡o collection health_check
        """
        self.collection = Collection(self.collection_name)
        self.collection.load()
    
    def _init_tool_collection(self) -> None:
        """
        Khá»Ÿi táº¡o collection tool
        """
        self.collection = Collection(self.collection_name)
        self.collection.load()
        
    def _init_collection(self, recreate: bool = False) -> None:
        """
        Khá»Ÿi táº¡o collection trong Milvus
        
        Args:
            recreate: CÃ³ táº¡o láº¡i collection náº¿u Ä‘Ã£ tá»“n táº¡i khÃ´ng
        """
        # Kiá»ƒm tra collection Ä‘Ã£ tá»“n táº¡i chÆ°a
        if utility.has_collection(self.collection_name):
            if recreate:
                utility.drop_collection(self.collection_name)
            else:
                self.collection = Collection(self.collection_name)
                self.collection.load()
                self.collection = Collection(self.collection_name)
                print(f"Collection '{self.collection_name}' Ä‘Ã£ tá»“n táº¡i.")
                return
            
        if self.collection_name == "inventory_item":
            self._init_inventory_item_collection()
            return

        if self.collection_name == "tool":
            self._init_inventory_item_collection()
            return
        
        if self.collection_name == "health_check":
            self._init_inventory_item_collection()
            return

        print(f"Táº¡o collection má»›i: {self.collection_name}")
        # XÃ¡c Ä‘á»‹nh sá»‘ chiá»u cá»§a vector TEXT (náº¿u cÃ³ embedding function)
        text_dim = self._get_vector_dim()
        if text_dim <= 0 and self.embedding_function:
             # Cá»‘ gáº¯ng láº¥y dimension tá»« cáº¥u hÃ¬nh embedding náº¿u cÃ³
             if hasattr(self.embedding_function, 'client') and hasattr(self.embedding_function.client, 'dimensions'):
                 text_dim = self.embedding_function.client.dimensions
             else:
                 # Náº¿u váº«n khÃ´ng Ä‘Æ°á»£c, Ä‘áº·t giÃ¡ trá»‹ máº·c Ä‘á»‹nh hoáº·c bÃ¡o lá»—i nghiÃªm trá»ng hÆ¡n
                 print(f"Error: KhÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh sá»‘ chiá»u cho vector text '{self.text_vector_field}'.")
                 # CÃ³ thá»ƒ raise lá»—i á»Ÿ Ä‘Ã¢y náº¿u trÆ°á»ng text vector lÃ  báº¯t buá»™c
                 text_dim = 768 # Hoáº·c má»™t giÃ¡ trá»‹ máº·c Ä‘á»‹nh an toÃ n khÃ¡c

        # --- Äá»‹nh nghÄ©a schema tá»‘i Æ°u ---
        fields = []

        # TrÆ°á»ng ID vá»›i kÃ­ch thÆ°á»›c há»£p lÃ½
        fields.append(FieldSchema(name=self.id_field, dtype=DataType.VARCHAR, is_primary=True, max_length=36))

        # TrÆ°á»ng text vá»›i kÃ­ch thÆ°á»›c há»£p lÃ½ hÆ¡n
        fields.append(FieldSchema(name=self.text_field, dtype=DataType.VARCHAR, max_length=1000))
        
        # Vector field
        fields.append(FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=text_dim))

        # Chá»‰ thÃªm metadata JSON duy nháº¥t - loáº¡i bá» cÃ¡c trÆ°á»ng dÆ° thá»«a
        fields.append(FieldSchema(name="metadata", dtype=DataType.JSON, is_nullable=True))

        # Chá»‰ thÃªm cÃ¡c trÆ°á»ng metadata tá»« metadata_fields náº¿u khÃ´ng trÃ¹ng vá»›i 'metadata'
        for field_name, field_type in self.metadata_fields:
            if field_name != "metadata":  # TrÃ¡nh trÃ¹ng láº·p
                defined_field_names = [f.name for f in fields]
                if field_name not in defined_field_names:
                     actual_field_type = field_type
                     if isinstance(field_type, str):
                          try:
                               actual_field_type = getattr(DataType, field_type.upper())
                          except AttributeError:
                               print(f"Warning: Kiá»ƒu dá»¯ liá»‡u metadata khÃ´ng há»£p lá»‡ '{field_type}' cho trÆ°á»ng '{field_name}'. Bá» qua.")
                               continue

                     if isinstance(actual_field_type, DataType):
                         if actual_field_type == DataType.VARCHAR:
                              fields.append(FieldSchema(name=field_name, dtype=actual_field_type, max_length=1024))  # Giáº£m max_length
                         else:
                              fields.append(FieldSchema(name=field_name, dtype=actual_field_type))

        # Táº¡o schema vÃ  collection
        try:
            schema = CollectionSchema(fields, enable_dynamic_field=False)
            self.collection = Collection(self.collection_name, schema)
            print(f"ÄÃ£ táº¡o collection '{self.collection_name}' vá»›i schema tá»‘i Æ°u.")
        except Exception as e:
            print(f"Lá»—i khi táº¡o collection '{self.collection_name}': {e}")
            raise

        # Sá»¬ Dá»¤NG METHOD DUY NHáº¤T Ä‘á»ƒ táº¡o index
        self._create_optimized_index("vector")

        # KHÃ”NG load collection ngay - sáº½ load sau khi insert xong
        print(f"Collection '{self.collection_name}' Ä‘Ã£ Ä‘Æ°á»£c táº¡o. Sáº½ load sau khi insert dá»¯ liá»‡u.")
    
    def add_texts(
        self, 
        texts: List[str], 
        metadatas: Optional[List[Dict[str, Any]]] = None,
        batch_size: int = 5000,  # TÄƒng batch_size máº·c Ä‘á»‹nh
        auto_flush: bool = True,  # ThÃªm tham sá»‘ auto_flush
        **kwargs
    ) -> List[str]:
        """
        ThÃªm vÄƒn báº£n vÃ  metadata vÃ o vector store vá»›i batch processing tá»‘i Æ°u
        
        Args:
            texts: Danh sÃ¡ch cÃ¡c vÄƒn báº£n cáº§n thÃªm
            metadatas: Danh sÃ¡ch metadata tÆ°Æ¡ng á»©ng vá»›i má»—i vÄƒn báº£n
            batch_size: KÃ­ch thÆ°á»›c batch Ä‘á»ƒ insert (máº·c Ä‘á»‹nh 5000)
            auto_flush: CÃ³ tá»± Ä‘á»™ng flush sau khi insert khÃ´ng (máº·c Ä‘á»‹nh True)
            
        Returns:
            List[str]: Danh sÃ¡ch ID cá»§a cÃ¡c vÄƒn báº£n Ä‘Ã£ thÃªm
        """
        import time
        start_time = time.time()
        
        print(f"ğŸš€ Báº¯t Ä‘áº§u add_texts: {len(texts):,} records vá»›i batch_size={batch_size:,}")
        
        # Táº¡o ID náº¿u khÃ´ng Ä‘Æ°á»£c cung cáº¥p
        ids = kwargs.get("ids", [str(uuid.uuid4()) for _ in range(len(texts))])
        
        # Táº¡o metadata náº¿u khÃ´ng Ä‘Æ°á»£c cung cáº¥p
        if metadatas is None:
            metadatas = [{} for _ in texts]
        
        # Äáº£m báº£o collection Ä‘Ã£ Ä‘Æ°á»£c load
        collection_load_start = time.time()
        if not hasattr(self.collection, '_loaded') or not self.collection._loaded:
            try:
                self.collection.load()
                print(f"ÄÃ£ load collection '{self.collection_name}' Ä‘á»ƒ insert dá»¯ liá»‡u.")
            except Exception as e:
                print(f"Warning: KhÃ´ng thá»ƒ load collection: {e}")
        collection_load_time = time.time() - collection_load_start
        
        all_ids = []
        total_batches = (len(texts) + batch_size - 1) // batch_size
        embedding_time = 0
        insert_time = 0
        
        # Insert theo batch Ä‘á»ƒ tá»‘i Æ°u hiá»‡u suáº¥t
        for i in range(0, len(texts), batch_size):
            batch_num = i // batch_size + 1
            batch_texts = texts[i:i + batch_size]
            batch_metadatas = metadatas[i:i + batch_size]
            batch_ids = ids[i:i + batch_size]
            
            # Progress info
            progress = (batch_num / total_batches) * 100
            print(f"\nğŸ“¦ BATCH {batch_num}/{total_batches} ({progress:.1f}%)")
            print(f"   ğŸ“ Processing: {i+1:,} â†’ {min(i+len(batch_texts), len(texts)):,} cá»§a {len(texts):,}")
            
            # Äo thá»i gian embedding - WITH BATCH EMBEDDING
            embedding_start = time.time()
            print(f"   ğŸ§  Batch embedding {len(batch_texts):,} texts...")
            
            # Sá»¬ Dá»¤NG BATCH EMBEDDING - NHANH HÆ N NHIá»€U Láº¦N!
            try:
                # Sá»­ dá»¥ng helper method Ä‘á»ƒ batch embed
                batch_vectors = self._batch_embed_texts(batch_texts, embedding_batch_size=400)
                print(f"   âœ… BATCH EMBEDDING thÃ nh cÃ´ng ({len(batch_vectors)} vectors)!")
                    
            except Exception as batch_error:
                print(f"   âš ï¸  Batch embedding failed: {batch_error}, fallback to single...")
                # Fallback vá» single embedding
                batch_vectors = []
                embedding_checkpoint = max(1, len(batch_texts) // 5)
                
                for j, text in enumerate(batch_texts):
                    vector = self.embedding_function(text)
                    batch_vectors.append(vector)
                    
                    if (j + 1) % embedding_checkpoint == 0 or j == len(batch_texts) - 1:
                        embed_progress = ((j + 1) / len(batch_texts)) * 100
                        embed_elapsed = time.time() - embedding_start
                        embed_speed = (j + 1) / embed_elapsed if embed_elapsed > 0 else 0
                        
                        print(f"      ğŸ“Š {j+1:,}/{len(batch_texts):,} ({embed_progress:.0f}%) | "
                              f"Speed: {embed_speed:.0f}/s")
            
            batch_embedding_time = time.time() - embedding_start
            embedding_time += batch_embedding_time
            embedding_speed = len(batch_texts) / batch_embedding_time if batch_embedding_time > 0 else 0
            
            print(f"   âœ… Embedding: {batch_embedding_time:.2f}s ({embedding_speed:.0f} texts/s)")
            
            # Chuáº©n bá»‹ dá»¯ liá»‡u Ä‘á»ƒ chÃ¨n (chá»‰ cÃ¡c trÆ°á»ng cáº§n thiáº¿t)
            data_prep_start = time.time()
            print(f"   ğŸ”§ Chuáº©n bá»‹ insert data...")
            data = [
                batch_ids,           # ID field
                batch_texts,         # Text field  
                batch_vectors,       # Vector field
                batch_metadatas,     # Metadata field
            ]
            data_prep_time = time.time() - data_prep_start
            print(f"   âœ… Data prep: {data_prep_time:.3f}s")
            
            # Äo thá»i gian insert - WITH DETAILED LOGGING
            insert_start = time.time()
            print(f"   ğŸ’¾ Inserting {len(batch_texts):,} records...")
            try:
                insert_result = self.collection.insert(data)
                all_ids.extend(batch_ids)
                batch_insert_time = time.time() - insert_start
                insert_time += batch_insert_time
                
                # Insert performance details
                insert_speed = len(batch_texts) / batch_insert_time if batch_insert_time > 0 else 0
                data_size_mb = (len(batch_texts) * 1000) / (1024 * 1024)  # Rough estimate
                
                print(f"   âœ… Insert: {batch_insert_time:.2f}s ({insert_speed:.0f} rec/s)")
                print(f"      ğŸ“Š ~{data_size_mb:.1f}MB | IDs: {len(insert_result.primary_keys):,}")
                
                # Batch summary
                batch_total_time = batch_embedding_time + batch_insert_time + data_prep_time
                records_per_sec = len(batch_texts) / batch_total_time if batch_total_time > 0 else 0
                
                print(f"   ğŸ¯ BATCH TOTAL: {batch_total_time:.2f}s | Speed: {records_per_sec:.0f} rec/s")
                print(f"      ğŸ“ˆ Progress: {len(all_ids):,}/{len(texts):,} completed")
                
                # ETA calculation
                if batch_num > 1:
                    avg_batch_time = (embedding_time + insert_time) / batch_num
                    remaining_batches = total_batches - batch_num
                    eta_seconds = remaining_batches * avg_batch_time
                    eta_minutes = eta_seconds / 60
                    
                    print(f"      ğŸ• ETA: {eta_minutes:.1f} phÃºt")
                    
                    # Mini progress bar
                    bar_length = 20
                    filled = int(bar_length * progress / 100)
                    bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
                    print(f"      ğŸ“Š [{bar}] {progress:.1f}%")
                
            except Exception as e:
                print(f"   âŒ Lá»—i insert batch {batch_num}: {e}")
                raise
        
        # Äo thá»i gian flush
        flush_start = time.time()
        if auto_flush:
            try:
                self.collection.flush()
                flush_time = time.time() - flush_start
                print(f"âœ… ÄÃ£ flush {len(all_ids):,} records trong {flush_time:.2f}s")
            except Exception as e:
                print(f"Warning: Lá»—i khi flush: {e}")
        else:
            flush_time = 0
        
        # TÃ­nh tá»•ng thá»i gian vÃ  thá»‘ng kÃª
        total_time = time.time() - start_time
        overall_speed = len(all_ids) / total_time
        
        print(f"ğŸ‰ HoÃ n thÃ nh add_texts!")
        print(f"ğŸ“Š THá»NG KÃŠ HIá»†U SUáº¤T:")
        print(f"   ğŸ“ Records: {len(all_ids):,}/{len(texts):,}")
        print(f"   â° Tá»•ng thá»i gian: {total_time:.2f}s")
        print(f"   ğŸ”„ Collection load: {collection_load_time:.2f}s")
        print(f"   ğŸ§  Embedding: {embedding_time:.2f}s ({embedding_time/total_time*100:.1f}%)")
        print(f"   ğŸ’¾ Insert: {insert_time:.2f}s ({insert_time/total_time*100:.1f}%)")
        print(f"   ğŸš€ Flush: {flush_time:.2f}s ({flush_time/total_time*100:.1f}%)")
        print(f"   âš¡ Tá»‘c Ä‘á»™: {overall_speed:.0f} records/second")
        
        return all_ids
    
    def similarity_search(
        self, 
        query: str, 
        k: int = 4, 
        **kwargs
    ) -> List[Tuple[Any, float]]:
        """
        TÃ¬m kiáº¿m vÄƒn báº£n tÆ°Æ¡ng tá»± vá»›i query dá»±a trÃªn Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng vá» vector
        
        Args:
            query: CÃ¢u truy váº¥n
            k: Sá»‘ lÆ°á»£ng káº¿t quáº£ tráº£ vá»
            
        Returns:
            List[Tuple[Any, float]]: Danh sÃ¡ch tuple gá»“m tÃ i liá»‡u vÃ  Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng
        """
        # Táº¡o vector tá»« query
        query_vector = self.embedding_function(query)
        
        # Thá»±c hiá»‡n tÃ¬m kiáº¿m
        search_params = {"metric_type": "COSINE", "params": {"ef": 64}}
        
        expr = None
        
        results = self.collection.search(
            data=[query_vector],
            anns_field=self.text_vector_field,
            param=search_params,
            limit=k,
            expr=expr,
            output_fields=[self.text_field, "metadata"]
        )
        
        # Chuyá»ƒn Ä‘á»•i káº¿t quáº£ sang Ä‘á»‹nh dáº¡ng tráº£ vá»
        docs_with_scores = []
        for hit in results[0]:
            doc = {
                "id": hit.id,
                "text": hit.entity.get(self.text_field),
                "metadata": hit.entity.get("metadata")
            }
            docs_with_scores.append((doc, hit.score))
        
        return docs_with_scores
    
    def fulltext_search(
        self, 
        query: str, 
        k: int = 4, 
        **kwargs
    ) -> List[Tuple[Any, float]]:
        """
        TÃ¬m kiáº¿m vÄƒn báº£n dá»±a trÃªn full-text search
        
        Args:
            query: CÃ¢u truy váº¥n
            k: Sá»‘ lÆ°á»£ng káº¿t quáº£ tráº£ vá»
            
        Returns:
            List[Tuple[Any, float]]: Danh sÃ¡ch tuple gá»“m tÃ i liá»‡u vÃ  Ä‘iá»ƒm sá»‘
        """
        try:
            # Thá»±c hiá»‡n full-text search
            expr = f'{self.text_field} like "%{query}%"'
            
            results = self.collection.query(
                expr=expr,
                output_fields=[self.id_field, self.text_field, "metadata"],
                limit=k
            )
            
            # TÃ­nh toÃ¡n Ä‘iá»ƒm sá»‘ Ä‘Æ¡n giáº£n dá»±a trÃªn sá»‘ láº§n xuáº¥t hiá»‡n cá»§a tá»« khÃ³a
            docs_with_scores = []
            query_words = query.lower().split()
            
            for item in results:
                text = item.get(self.text_field, "").lower()
                score = 0
                for word in query_words:
                    score += text.count(word)
                
                # Chuáº©n hÃ³a Ä‘iá»ƒm sá»‘
                if len(query_words) > 0:
                    score = score / len(query_words)
                
                doc = {
                    "id": item.get(self.id_field),
                    "text": item.get(self.text_field),
                    "metadata": item.get("metadata", {})
                }
                docs_with_scores.append((doc, min(score, 1.0)))
            
            # Sáº¯p xáº¿p káº¿t quáº£ theo Ä‘iá»ƒm sá»‘ giáº£m dáº§n
            docs_with_scores.sort(key=lambda x: x[1], reverse=True)
            return docs_with_scores[:k]
            
        except Exception as e:
            print(f"Lá»—i khi thá»±c hiá»‡n fulltext search: {e}")
            return []
    
    def hybrid_search(
        self, 
        query: str, 
        k: int = 4, 
        alpha: float = 0.5, 
        **kwargs
    ) -> List[Tuple[Any, float]]:
        """
        TÃ¬m kiáº¿m vÄƒn báº£n káº¿t há»£p giá»¯a vector similarity vÃ  full-text search
        
        Args:
            query: CÃ¢u truy váº¥n
            k: Sá»‘ lÆ°á»£ng káº¿t quáº£ tráº£ vá»
            alpha: Trá»ng sá»‘ cho káº¿t quáº£ vector similarity (0-1)
            
        Returns:
            List[Tuple[Any, float]]: Danh sÃ¡ch tuple gá»“m tÃ i liá»‡u vÃ  Ä‘iá»ƒm sá»‘ káº¿t há»£p
        """
        # Láº¥y káº¿t quáº£ tá»« cáº£ hai phÆ°Æ¡ng phÃ¡p
        vector_results = self.similarity_search(query, k=k*2, **kwargs)
        fulltext_results = self.fulltext_search(query, k=k*2, **kwargs)
        
        # Táº¡o map tá»« ID Ä‘áº¿n káº¿t quáº£ vÃ  Ä‘iá»ƒm sá»‘
        results_map = {}
        
        # ThÃªm káº¿t quáº£ vector search vá»›i trá»ng sá»‘ alpha
        for doc, score in vector_results:
            doc_id = doc["id"]
            if doc_id not in results_map:
                results_map[doc_id] = {"doc": doc, "vector_score": 0, "text_score": 0}
            results_map[doc_id]["vector_score"] = score
        
        # ThÃªm káº¿t quáº£ fulltext search vá»›i trá»ng sá»‘ (1-alpha)
        for doc, score in fulltext_results:
            doc_id = doc["id"]
            if doc_id not in results_map:
                results_map[doc_id] = {"doc": doc, "vector_score": 0, "text_score": 0}
            results_map[doc_id]["text_score"] = score
        
        # TÃ­nh Ä‘iá»ƒm káº¿t há»£p
        hybrid_results = []
        for doc_id, result in results_map.items():
            hybrid_score = alpha * result["vector_score"] + (1 - alpha) * result["text_score"]
            hybrid_results.append((result["doc"], hybrid_score))
        
        # Sáº¯p xáº¿p theo Ä‘iá»ƒm káº¿t há»£p giáº£m dáº§n
        hybrid_results.sort(key=lambda x: x[1], reverse=True)
        
        return hybrid_results[:k]
    
    def delete(self, ids: List[str], **kwargs) -> None:
        """
        XÃ³a cÃ¡c vÄƒn báº£n khá»i vector store dá»±a trÃªn ID
        
        Args:
            ids: Danh sÃ¡ch ID cáº§n xÃ³a
        """
        expr = f"{self.id_field} in {ids}"
        self.collection.delete(expr)
        self.collection.flush()
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """
        Láº¥y thÃ´ng tin thá»‘ng kÃª vá» collection
        
        Returns:
            Dict[str, Any]: ThÃ´ng tin thá»‘ng kÃª
        """
        stats = {
            "name": self.collection_name,
            "count": self.collection.num_entities,
            "fields": self.collection.schema.fields,
        }
        
        return stats

    def optimize_collection(self) -> None:
        """
        Tá»‘i Æ°u hÃ³a collection sau khi insert dá»¯ liá»‡u xong
        - Compact Ä‘á»ƒ giáº£m phÃ¢n máº£nh
        - Rebuild index náº¿u cáº§n
        """
        try:
            print(f"Äang tá»‘i Æ°u hÃ³a collection '{self.collection_name}'...")
            
            # Flush Ä‘á»ƒ Ä‘áº£m báº£o táº¥t cáº£ dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c ghi
            self.collection.flush()
            
            # Compact Ä‘á»ƒ giáº£m phÃ¢n máº£nh
            self.collection.compact()
            print("ÄÃ£ thá»±c hiá»‡n compact collection.")
            
            # Kiá»ƒm tra vÃ  rebuild index náº¿u cáº§n
            index_info = self.collection.indexes
            if index_info:
                print("Index Ä‘Ã£ tá»“n táº¡i, khÃ´ng cáº§n rebuild.")
            else:
                print("Äang rebuild index...")
                # Sá»¬ Dá»¤NG METHOD DUY NHáº¤T Ä‘á»ƒ táº¡o index
                self._create_optimized_index("vector")
            
            # Load collection Ä‘á»ƒ sá»­ dá»¥ng
            self.collection.load()
            print(f"Collection '{self.collection_name}' Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a vÃ  load.")
            
        except Exception as e:
            print(f"Lá»—i khi tá»‘i Æ°u hÃ³a collection: {e}")

    def bulk_insert_texts(
        self, 
        texts: List[str], 
        metadatas: Optional[List[Dict[str, Any]]] = None,
        batch_size: int = 10000,  # TÄƒng batch size cho bulk insert
        **kwargs
    ) -> List[str]:
        """
        Insert hÃ ng loáº¡t vá»›i tá»‘i Æ°u hÃ³a cao nháº¥t
        
        Args:
            texts: Danh sÃ¡ch vÄƒn báº£n
            metadatas: Danh sÃ¡ch metadata
            batch_size: KÃ­ch thÆ°á»›c batch (máº·c Ä‘á»‹nh 10000)
            
        Returns:
            List[str]: Danh sÃ¡ch ID Ä‘Ã£ insert
        """
        import time
        start_time = time.time()
        
        print(f"ğŸš€ Báº¯t Ä‘áº§u BULK INSERT: {len(texts):,} records vá»›i batch_size={batch_size:,}")
        
        # Insert táº¥t cáº£ batch mÃ  khÃ´ng flush
        insert_start = time.time()
        all_ids = self.add_texts(
            texts=texts, 
            metadatas=metadatas, 
            batch_size=batch_size,
            auto_flush=False,  # KhÃ´ng flush tá»«ng batch
            **kwargs
        )
        insert_time = time.time() - insert_start
        
        # Flush má»™t láº§n duy nháº¥t á»Ÿ cuá»‘i
        flush_start = time.time()
        print("ğŸ”„ Äang flush táº¥t cáº£ dá»¯ liá»‡u...")
        self.collection.flush()
        flush_time = time.time() - flush_start
        
        # Tá»‘i Æ°u hÃ³a collection
        optimize_start = time.time()
        print("âš¡ Äang tá»‘i Æ°u hÃ³a collection...")
        self.optimize_collection()
        optimize_time = time.time() - optimize_start
        
        # TÃ­nh tá»•ng thá»i gian
        total_time = time.time() - start_time
        overall_speed = len(all_ids) / total_time
        
        print(f"ğŸ‰ HOÃ€N THÃ€NH BULK INSERT!")
        print(f"ğŸ“Š THá»NG KÃŠ Tá»”NG QUAN:")
        print(f"   ğŸ“ Records: {len(all_ids):,}")
        print(f"   â° Tá»•ng thá»i gian: {total_time:.2f}s")
        print(f"   ğŸ’¾ Insert + Embedding: {insert_time:.2f}s ({insert_time/total_time*100:.1f}%)")
        print(f"   ğŸš€ Final Flush: {flush_time:.2f}s ({flush_time/total_time*100:.1f}%)")
        print(f"   âš™ï¸  Optimization: {optimize_time:.2f}s ({optimize_time/total_time*100:.1f}%)")
        print(f"   âš¡ Tá»‘c Ä‘á»™ tá»•ng: {overall_speed:.0f} records/second")
        
        return all_ids

    def mega_insert_texts(
        self, 
        texts: List[str], 
        metadatas: Optional[List[Dict[str, Any]]] = None,
        batch_size: int = 50000,  # Batch ráº¥t lá»›n cho mega insert
        **kwargs
    ) -> List[str]:
        """
        Insert siÃªu lá»›n cho hÃ ng triá»‡u báº£n ghi vá»›i tá»‘i Æ°u hÃ³a tá»‘i Ä‘a
        
        Args:
            texts: Danh sÃ¡ch vÄƒn báº£n
            metadatas: Danh sÃ¡ch metadata  
            batch_size: KÃ­ch thÆ°á»›c batch (máº·c Ä‘á»‹nh 50,000)
            
        Returns:
            List[str]: Danh sÃ¡ch ID Ä‘Ã£ insert
        """
        import time
        start_time = time.time()
        
        total_records = len(texts)
        print(f"ğŸš€ Báº¯t Ä‘áº§u MEGA INSERT {total_records:,} records vá»›i batch_size={batch_size:,}")
        
        if total_records > 1000000:  # > 1 triá»‡u
            print("âš ï¸  Cáº¢NH BÃO: Insert hÆ¡n 1 triá»‡u records. Äáº£m báº£o Ä‘á»§ RAM vÃ  thá»i gian!")
        
        # Táº¡o ID náº¿u khÃ´ng Ä‘Æ°á»£c cung cáº¥p
        preparation_start = time.time()
        ids = kwargs.get("ids", [str(uuid.uuid4()) for _ in range(total_records)])
        
        # Táº¡o metadata náº¿u khÃ´ng Ä‘Æ°á»£c cung cáº¥p
        if metadatas is None:
            metadatas = [{} for _ in texts]
        preparation_time = time.time() - preparation_start
        
        # Äáº£m báº£o collection Ä‘Ã£ Ä‘Æ°á»£c load
        load_start = time.time()
        if not hasattr(self.collection, '_loaded') or not self.collection._loaded:
            try:
                self.collection.load()
                print(f"ÄÃ£ load collection '{self.collection_name}' Ä‘á»ƒ insert dá»¯ liá»‡u.")
            except Exception as e:
                print(f"Warning: KhÃ´ng thá»ƒ load collection: {e}")
        load_time = time.time() - load_start
        
        all_ids = []
        total_batches = (total_records + batch_size - 1) // batch_size
        total_embedding_time = 0
        total_insert_time = 0
        
        # Insert theo batch siÃªu lá»›n
        for i in range(0, total_records, batch_size):
            batch_num = i // batch_size + 1
            batch_texts = texts[i:i + batch_size]
            batch_metadatas = metadatas[i:i + batch_size]
            batch_ids = ids[i:i + batch_size]
            
            batch_start = time.time()
            progress = (batch_num / total_batches) * 100
            print(f"\nğŸ“¦ BATCH {batch_num}/{total_batches} ({progress:.1f}%) - {len(batch_texts):,} records")
            print(f"   ğŸ“ Records: {i+1:,} â†’ {min(i+len(batch_texts), total_records):,}")
            
            # Táº¡o vectors tá»« batch texts - WITH BATCH EMBEDDING
            embedding_start = time.time()
            print(f"   ğŸ§  Batch embedding {len(batch_texts):,} texts...")
            
            # Sá»¬ Dá»¤NG BATCH EMBEDDING - SIÃŠU NHANH!
            try:
                # Sá»­ dá»¥ng helper method Ä‘á»ƒ batch embed
                batch_vectors = self._batch_embed_texts(batch_texts, embedding_batch_size=400)
                print(f"   ğŸš€ BATCH EMBEDDING thÃ nh cÃ´ng ({len(batch_vectors)} vectors) - siÃªu tá»‘i Æ°u!")
                    
            except Exception as batch_error:
                print(f"   âš ï¸  Batch embedding error: {batch_error}, fallback...")
                # Fallback to single embedding vá»›i progress tracking
                batch_vectors = []
                embedding_checkpoint = max(1, len(batch_texts) // 4)
                
                for j, text in enumerate(batch_texts):
                    vector = self.embedding_function(text)
                    batch_vectors.append(vector)
                    
                    if (j + 1) % embedding_checkpoint == 0 or j == len(batch_texts) - 1:
                        embed_progress = ((j + 1) / len(batch_texts)) * 100
                        embed_elapsed = time.time() - embedding_start
                        embed_speed = (j + 1) / embed_elapsed
                        remaining_embeds = len(batch_texts) - (j + 1)
                        embed_eta = remaining_embeds / embed_speed if embed_speed > 0 else 0
                        
                        print(f"      ğŸ“Š Embedding: {j+1:,}/{len(batch_texts):,} ({embed_progress:.0f}%) | "
                              f"Speed: {embed_speed:.0f}/s | ETA: {embed_eta:.1f}s")
            
            embedding_time = time.time() - embedding_start
            total_embedding_time += embedding_time
            embedding_speed = len(batch_texts) / embedding_time
            
            print(f"   âœ… Embedding hoÃ n thÃ nh: {embedding_time:.1f}s ({embedding_speed:.0f} texts/s)")
            
            # Chuáº©n bá»‹ dá»¯ liá»‡u Ä‘á»ƒ chÃ¨n
            data_prep_start = time.time()
            print(f"   ğŸ”§ Chuáº©n bá»‹ dá»¯ liá»‡u Ä‘á»ƒ insert...")
            data = [
                batch_ids,           # ID field
                batch_texts,         # Text field  
                batch_vectors,       # Vector field
                batch_metadatas,     # Metadata field
            ]
            data_prep_time = time.time() - data_prep_start
            print(f"   âœ… Dá»¯ liá»‡u Ä‘Ã£ chuáº©n bá»‹: {data_prep_time:.2f}s")
            
            # ChÃ¨n batch vÃ o collection - WITH DETAILED LOGGING  
            insert_start = time.time()
            print(f"   ğŸ’¾ Báº¯t Ä‘áº§u insert {len(batch_texts):,} records vÃ o Milvus...")
            
            try:
                insert_result = self.collection.insert(data)
                all_ids.extend(batch_ids)
                insert_time = time.time() - insert_start
                total_insert_time += insert_time
                
                # Chi tiáº¿t vá» insert performance
                insert_speed = len(batch_texts) / insert_time
                data_size_mb = (len(batch_texts) * (1000 + 512 * 4)) / (1024 * 1024)  # Æ¯á»›c tÃ­nh MB
                
                print(f"   âœ… Insert hoÃ n thÃ nh: {insert_time:.1f}s ({insert_speed:.0f} rec/s)")
                print(f"      ğŸ“Š Data size: ~{data_size_mb:.1f}MB | Insert IDs: {len(insert_result.primary_keys):,}")
                
                # Overall batch performance
                batch_total_time = time.time() - batch_start
                batch_speed = len(batch_texts) / batch_total_time
                
                print(f"   ğŸ¯ BATCH SUMMARY:")
                print(f"      â±ï¸  Total: {batch_total_time:.1f}s | Embedding: {embedding_time:.1f}s ({embedding_time/batch_total_time*100:.0f}%) | Insert: {insert_time:.1f}s ({insert_time/batch_total_time*100:.0f}%)")
                print(f"      âš¡ Speed: {batch_speed:.0f} rec/s total | {len(all_ids):,}/{total_records:,} completed")
                
                # Æ¯á»›c tÃ­nh thá»i gian cÃ²n láº¡i - IMPROVED ETA
                if batch_num > 1:
                    avg_time_per_batch = (time.time() - start_time - preparation_time - load_time) / batch_num
                    remaining_batches = total_batches - batch_num
                    eta_seconds = remaining_batches * avg_time_per_batch
                    eta_minutes = eta_seconds / 60
                    eta_hours = eta_minutes / 60
                    
                    if eta_hours >= 1:
                        print(f"      ğŸ• ETA: {eta_hours:.1f} giá» ({eta_minutes:.0f} phÃºt)")
                    else:
                        print(f"      ğŸ• ETA: {eta_minutes:.1f} phÃºt")
                    
                    # Progress bar visual
                    bar_length = 30
                    filled_length = int(bar_length * progress / 100)
                    bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                    print(f"      ğŸ“ˆ [{bar}] {progress:.1f}%")
                
            except Exception as e:
                print(f"   âŒ Lá»—i khi insert batch {batch_num}: {e}")
                raise
        
        # Flush má»™t láº§n duy nháº¥t á»Ÿ cuá»‘i
        flush_start = time.time()
        print("ğŸ”„ Äang flush táº¥t cáº£ dá»¯ liá»‡u...")
        self.collection.flush()
        flush_time = time.time() - flush_start
        
        # Tá»‘i Æ°u hÃ³a collection
        optimize_start = time.time()
        print("âš¡ Äang tá»‘i Æ°u hÃ³a collection...")
        self.optimize_collection()
        optimize_time = time.time() - optimize_start
        
        # TÃ­nh toÃ¡n thá»‘ng kÃª tá»•ng quan
        total_time = time.time() - start_time
        overall_speed = len(all_ids) / total_time
        
        print(f"ğŸ‰ HOÃ€N THÃ€NH MEGA INSERT {len(all_ids):,} records!")
        print(f"ğŸ“Š THá»NG KÃŠ MEGA INSERT:")
        print(f"   ğŸ“ Records: {len(all_ids):,}")
        print(f"   â° Tá»•ng thá»i gian: {total_time/60:.1f} phÃºt ({total_time:.1f}s)")
        print(f"   ğŸ”§ Preparation: {preparation_time:.1f}s ({preparation_time/total_time*100:.1f}%)")
        print(f"   ğŸ”„ Collection load: {load_time:.1f}s ({load_time/total_time*100:.1f}%)")
        print(f"   ğŸ§  Total Embedding: {total_embedding_time:.1f}s ({total_embedding_time/total_time*100:.1f}%)")
        print(f"   ğŸ’¾ Total Insert: {total_insert_time:.1f}s ({total_insert_time/total_time*100:.1f}%)")
        print(f"   ğŸš€ Final Flush: {flush_time:.1f}s ({flush_time/total_time*100:.1f}%)")
        print(f"   âš™ï¸  Optimization: {optimize_time:.1f}s ({optimize_time/total_time*100:.1f}%)")
        print(f"   âš¡ Tá»‘c Ä‘á»™ trung bÃ¬nh: {overall_speed:.0f} records/second")
        print(f"   ğŸ’° Chi phÃ­ thá»i gian trÃªn 1K records: {total_time/total_records*1000:.2f}s")
        
        return all_ids