"""
Module tri·ªÉn khai Milvus vector store
"""
import os
import uuid
from typing import Any, Dict, List, Optional, Union, Tuple
from pymilvus import Collection, connections, utility, FieldSchema, CollectionSchema
from pymilvus import DataType, MilvusException
from util.env_loader import get_env

from stores.base_vector_store import BaseVectorStore

class MilvusVectorStore(BaseVectorStore):
    """
    Tri·ªÉn khai vector store s·ª≠ d·ª•ng Milvus
    """
    
    def __init__(
        self,
        collection_name: str,
        embedding_function: Any,
        uri: Optional[str] = None,
        db_name: Optional[str] = None,
        user: Optional[str] = None,
        password: Optional[str] = None,
        text_field: str = "text", # Field for text content
        # vector_field is removed, specific fields below
        id_field: str = "id",
        metadata_fields: Optional[List[Tuple[str, str]]] = None, # e.g., [("metadata", DataType.JSON)]
        clip_dimension: int = 512, # Default CLIP dimension
        resnet_dimension: int = 2048, # Default ResNet dimension
        text_vector_field: str = "vector", # Field name for text embeddings
        recreate_collection: bool = False,
        **kwargs
    ):
        """
        Kh·ªüi t·∫°o Milvus vector store
        
        Args:
            collection_name: T√™n c·ªßa collection
            embedding_function: H√†m embedding (primarily for text). Can be None if only adding precomputed vectors.
            uri: URI k·∫øt n·ªëi t·ªõi Milvus server. M·∫∑c ƒë·ªãnh l·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng MILVUS_URI
            db_name: T√™n database. M·∫∑c ƒë·ªãnh l·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng MILVUS_DB_NAME
            user: T√™n ng∆∞·ªùi d√πng. M·∫∑c ƒë·ªãnh l·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng MILVUS_USER
            password: M·∫≠t kh·∫©u. M·∫∑c ƒë·ªãnh l·∫•y t·ª´ bi·∫øn m√¥i tr∆∞·ªùng MILVUS_PASSWORD
            text_field: T√™n tr∆∞·ªùng ch·ª©a vƒÉn b·∫£n g·ªëc.
            id_field: T√™n tr∆∞·ªùng ch·ª©a ID ch√≠nh (primary key).
            metadata_fields: Danh s√°ch c√°c tr∆∞·ªùng metadata b·ªï sung v√† ki·ªÉu d·ªØ li·ªáu (e.g., [("metadata", DataType.JSON)]).
            clip_dimension: S·ªë chi·ªÅu cho vector CLIP.
            resnet_dimension: S·ªë chi·ªÅu cho vector ResNet.
            text_vector_field: T√™n tr∆∞·ªùng ƒë·ªÉ l∆∞u vector embedding c·ªßa text.
            recreate_collection: C√≥ t·∫°o l·∫°i collection n·∫øu ƒë√£ t·ªìn t·∫°i kh√¥ng
        """
        self.collection_name = collection_name
        self.embedding_function = embedding_function # Used for text embedding and potentially dim inference
        self.text_field = text_field
        self.id_field = id_field
        self.clip_dimension = clip_dimension
        self.resnet_dimension = resnet_dimension
        self.text_vector_field = text_vector_field
        
        # L·∫•y th√¥ng tin k·∫øt n·ªëi t·ª´ bi·∫øn m√¥i tr∆∞·ªùng n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
        self.uri = uri or get_env("MILVUS_URI", "http://localhost:19530")
        self.db_name = db_name or get_env("MILVUS_DB_NAME", "default")
        print(f"MILVUS_DB_NAME: {self.db_name}")
        self.user = user or get_env("MILVUS_USER", "")
        self.password = password or get_env("MILVUS_PASSWORD", "")
        
        # ƒê·ªãnh nghƒ©a c√°c tr∆∞·ªùng metadata
        self.metadata_fields = metadata_fields or [
            ("metadata", DataType.JSON)  # M·∫∑c ƒë·ªãnh c√≥ m·ªôt tr∆∞·ªùng metadata d·∫°ng JSON
        ]
        
        try:
            # K·∫øt n·ªëi t·ªõi Milvus server
            self._connect()
            
            # Ki·ªÉm tra v√† t·∫°o collection n·∫øu c·∫ßn
            self._init_collection(recreate_collection)
        except MilvusException as e:
            print(f"L·ªói k·∫øt n·ªëi Milvus: {e}")
            print(f"Ki·ªÉm tra l·∫°i c·∫•u h√¨nh k·∫øt n·ªëi:")
            print(f"- URI: {self.uri}")
            print(f"- Database: {self.db_name}")
            print(f"- User: {self.user}")
            raise
    
    def _connect(self) -> None:
        """
        K·∫øt n·ªëi t·ªõi Milvus server
        """
        try:
            # K·∫øt n·ªëi t·ªõi Milvus server v·ªõi database ƒë√£ ch·ªçn
            connections.connect(
                alias="default", 
                uri=self.uri,
                db_name=self.db_name,
                user=self.user,
                password=self.password
            )
            print(f"ƒê√£ k·∫øt n·ªëi t·ªõi database '{self.db_name}'")
        except Exception as e:
            # N·∫øu l·ªói l√† do database kh√¥ng t·ªìn t·∫°i, th·ª≠ t·∫°o m·ªõi
            if "database not found" in str(e).lower():
                # K·∫øt n·ªëi l·∫°i kh√¥ng ch·ªâ ƒë·ªãnh database
                connections.connect(
                    alias="default", 
                    uri=self.uri,
                    user=self.user,
                    password=self.password
                )
                # T·∫°o database m·ªõi
                from pymilvus import db
                db.create_database(self.db_name)
                print(f"ƒê√£ t·∫°o database '{self.db_name}'")
                
                # K·∫øt n·ªëi l·∫°i v·ªõi database m·ªõi
                connections.disconnect("default")
                connections.connect(
                    alias="default", 
                    uri=self.uri,
                    db_name=self.db_name,
                    user=self.user,
                    password=self.password
                )
                print(f"ƒê√£ k·∫øt n·ªëi t·ªõi database '{self.db_name}'")
            else:
                print(f"L·ªói khi k·∫øt n·ªëi: {e}")
                raise
    
    def _get_vector_dim(self) -> int:
        """
        L·∫•y s·ªë chi·ªÅu c·ªßa vector embedding
        
        Returns:
            int: S·ªë chi·ªÅu c·ªßa vector ho·∫∑c -1 n·∫øu kh√¥ng th·ªÉ x√°c ƒë·ªãnh.
        """
        if self.embedding_function:
            try:
                # T·∫°o m·ªôt vector t·ª´ vƒÉn b·∫£n m·∫´u ƒë·ªÉ x√°c ƒë·ªãnh s·ªë chi·ªÅu
                sample_vector = self.embedding_function("Sample text")
                if isinstance(sample_vector, list):
                    return len(sample_vector)
                elif hasattr(sample_vector, 'shape'):
                     return sample_vector.shape[0]
            except Exception as e:
                print(f"Warning: Kh√¥ng th·ªÉ x√°c ƒë·ªãnh s·ªë chi·ªÅu t·ª´ embedding_function: {e}")
        # Tr·∫£ v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh ho·∫∑c b√°o l·ªói n·∫øu kh√¥ng c√≥ embedding_function
        # Ho·∫∑c d·ª±a v√†o collection schema n·∫øu ƒë√£ t·ªìn t·∫°i? T·∫°m th·ªùi tr·∫£ v·ªÅ -1
        print("Warning: Kh√¥ng th·ªÉ x√°c ƒë·ªãnh s·ªë chi·ªÅu vector t·ª´ embedding_function.")
        return -1 # Ho·∫∑c m·ªôt gi√° tr·ªã m·∫∑c ƒë·ªãnh kh√°c / raise error
    
    def _create_optimized_index(self, field_name: str = "vector") -> None:
        """
        T·∫°o index t·ªëi ∆∞u cho vector field - SINGLE SOURCE OF TRUTH
        
        Args:
            field_name: T√™n field c·∫ßn ƒë√°nh index (m·∫∑c ƒë·ªãnh "vector")
        """
        # Tham s·ªë index t·ªëi ∆∞u duy nh·∫•t - KH√îNG duplicate n·ªØa
        index_params = {
            "metric_type": "COSINE", 
            "index_type": "HNSW", 
            "params": {
                "M": 16,             # T·ªëi ∆∞u cho ch·∫•t l∆∞·ª£ng v√† hi·ªáu su·∫•t
                "efConstruction": 200 # T·ªëi ∆∞u ƒë·ªÉ gi·∫£m ph√¢n m·∫£nh
            }
        }
        
        try:
            self.collection.create_index(field_name, index_params)
            print(f"‚úÖ ƒê√£ t·∫°o index t·ªëi ∆∞u cho field '{field_name}' (M=16, efConstruction=200)")
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫°o index cho '{field_name}': {e}")
            raise
    
    def _batch_embed_texts(self, texts: List[str], embedding_batch_size: int = 200) -> List[List[float]]:
        """
        Batch embedding v·ªõi sub-batches ƒë·ªÉ tr√°nh qu√° t·∫£i API
        
        Args:
            texts: Danh s√°ch texts c·∫ßn embed
            embedding_batch_size: K√≠ch th∆∞·ªõc sub-batch (m·∫∑c ƒë·ªãnh 200)
            
        Returns:
            List[List[float]]: Danh s√°ch vectors
        """
        import time  # Import time module
        
        if not texts:
            return []
            
        if len(texts) == 1:
            # Single text
            return [self.embedding_function(texts[0])]
        
        # Batch embedding v·ªõi sub-batches
        all_vectors = []
        total_sub_batches = (len(texts) + embedding_batch_size - 1) // embedding_batch_size
        
        print(f"      üß† Chia th√†nh {total_sub_batches} sub-batches ({embedding_batch_size} records/batch)...")
        
        for sub_i in range(0, len(texts), embedding_batch_size):
            sub_batch_texts = texts[sub_i:sub_i + embedding_batch_size]
            sub_batch_num = (sub_i // embedding_batch_size) + 1
            
            sub_start = time.time()
            print(f"         üîÑ Sub-batch {sub_batch_num}/{total_sub_batches}: {len(sub_batch_texts)} texts...")
            
            # G·ªçi embedding cho sub-batch
            sub_vectors = self.embedding_function(sub_batch_texts)
            all_vectors.extend(sub_vectors)
            
            sub_time = time.time() - sub_start
            sub_progress = (sub_batch_num / total_sub_batches) * 100
            sub_speed = len(sub_batch_texts) / sub_time if sub_time > 0 else 0
            
            print(f"         ‚úÖ Sub-batch {sub_batch_num} ho√†n th√†nh ({sub_progress:.0f}%) - {sub_time:.1f}s - {sub_speed:.0f} texts/s")
        
        return all_vectors
    
    def _init_inventory_item_collection(self) -> None:
        """
        Kh·ªüi t·∫°o collection inventory_item
        """
        # X√°c ƒë·ªãnh s·ªë chi·ªÅu c·ªßa vector
        dim = self._get_vector_dim()
        
        # ƒê·ªãnh nghƒ©a schema cho collection
        fields = [
            FieldSchema(name=self.id_field, dtype=DataType.VARCHAR, is_primary=True, max_length=100),
            FieldSchema(name=self.text_field, dtype=DataType.VARCHAR, max_length=65535),
            FieldSchema(name=self.text_vector_field, dtype=DataType.FLOAT_VECTOR, dim=dim),
            FieldSchema(name="image_id", dtype=DataType.VARCHAR, max_length=100),  # Th√™m tr∆∞·ªùng image_id ri√™ng
            FieldSchema(name="inventory_item_id", dtype=DataType.VARCHAR, max_length=100),
        ]
        
        # Th√™m c√°c tr∆∞·ªùng metadata
        for field_name, field_type in self.metadata_fields:
            fields.append(FieldSchema(name=field_name, dtype=field_type))
        
        # T·∫°o schema v√† collection
        try:
            schema = CollectionSchema(fields, enable_dynamic_field=False) # Kh√¥ng b·∫≠t dynamic field tr·ª´ khi th·ª±c s·ª± c·∫ßn
            self.collection = Collection(self.collection_name, schema)
            print(f"ƒê√£ t·∫°o collection '{self.collection_name}' v·ªõi schema.")
        except Exception as e:
            print(f"L·ªói khi t·∫°o collection '{self.collection_name}': {e}")
            raise # N√©m l·∫°i l·ªói ƒë·ªÉ d·ª´ng qu√° tr√¨nh n·∫øu kh√¥ng t·∫°o ƒë∆∞·ª£c collection

        # S·ª¨ D·ª§NG METHOD DUY NH·∫§T ƒë·ªÉ t·∫°o index
        self._create_optimized_index("vector")

        # Load collection v√†o memory sau khi t·∫°o index
        print(f"ƒêang load collection '{self.collection_name}'...")
        self.collection.load()
        print(f"Collection '{self.collection_name}' ƒë√£ ƒë∆∞·ª£c load.")
    
    def _init_health_check_collection(self) -> None:
        """
        Kh·ªüi t·∫°o collection health_check
        """
        self.collection = Collection(self.collection_name)
        self.collection.load()
    
    def _init_tool_collection(self) -> None:
        """
        Kh·ªüi t·∫°o collection tool
        """
        self.collection = Collection(self.collection_name)
        self.collection.load()
        
    def _init_collection(self, recreate: bool = False) -> None:
        """
        Kh·ªüi t·∫°o collection trong Milvus
        
        Args:
            recreate: C√≥ t·∫°o l·∫°i collection n·∫øu ƒë√£ t·ªìn t·∫°i kh√¥ng
        """
        # Ki·ªÉm tra collection ƒë√£ t·ªìn t·∫°i ch∆∞a
        if utility.has_collection(self.collection_name):
            if recreate:
                utility.drop_collection(self.collection_name)
            else:
                self.collection = Collection(self.collection_name)
                self.collection.load()
                self.collection = Collection(self.collection_name)
                print(f"Collection '{self.collection_name}' ƒë√£ t·ªìn t·∫°i.")
                return
            
        if self.collection_name == "inventory_item":
            self._init_inventory_item_collection()
            return

        if self.collection_name == "tool":
            self._init_inventory_item_collection()
            return
        
        if self.collection_name == "health_check":
            self._init_inventory_item_collection()
            return

        print(f"T·∫°o collection m·ªõi: {self.collection_name}")
        # X√°c ƒë·ªãnh s·ªë chi·ªÅu c·ªßa vector TEXT (n·∫øu c√≥ embedding function)
        text_dim = self._get_vector_dim()
        if text_dim <= 0 and self.embedding_function:
             # C·ªë g·∫Øng l·∫•y dimension t·ª´ c·∫•u h√¨nh embedding n·∫øu c√≥
             if hasattr(self.embedding_function, 'client') and hasattr(self.embedding_function.client, 'dimensions'):
                 text_dim = self.embedding_function.client.dimensions
             else:
                 # N·∫øu v·∫´n kh√¥ng ƒë∆∞·ª£c, ƒë·∫∑t gi√° tr·ªã m·∫∑c ƒë·ªãnh ho·∫∑c b√°o l·ªói nghi√™m tr·ªçng h∆°n
                 print(f"Error: Kh√¥ng th·ªÉ x√°c ƒë·ªãnh s·ªë chi·ªÅu cho vector text '{self.text_vector_field}'.")
                 # C√≥ th·ªÉ raise l·ªói ·ªü ƒë√¢y n·∫øu tr∆∞·ªùng text vector l√† b·∫Øt bu·ªôc
                 text_dim = 768 # Ho·∫∑c m·ªôt gi√° tr·ªã m·∫∑c ƒë·ªãnh an to√†n kh√°c

        # --- ƒê·ªãnh nghƒ©a schema t·ªëi ∆∞u ---
        fields = []

        # Tr∆∞·ªùng ID v·ªõi k√≠ch th∆∞·ªõc h·ª£p l√Ω
        fields.append(FieldSchema(name=self.id_field, dtype=DataType.VARCHAR, is_primary=True, max_length=36))

        # Tr∆∞·ªùng text v·ªõi k√≠ch th∆∞·ªõc h·ª£p l√Ω h∆°n
        fields.append(FieldSchema(name=self.text_field, dtype=DataType.VARCHAR, max_length=1000))
        
        # Vector field
        fields.append(FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=text_dim))

        # Th√™m tr∆∞·ªùng image_id ri√™ng ƒë·ªÉ l∆∞u ID g·ªëc c·ªßa image
        fields.append(FieldSchema(name="image_id", dtype=DataType.VARCHAR, max_length=100))

        # Ch·ªâ th√™m metadata JSON duy nh·∫•t - lo·∫°i b·ªè c√°c tr∆∞·ªùng d∆∞ th·ª´a
        fields.append(FieldSchema(name="metadata", dtype=DataType.JSON, is_nullable=True))

        # Ch·ªâ th√™m c√°c tr∆∞·ªùng metadata t·ª´ metadata_fields n·∫øu kh√¥ng tr√πng v·ªõi 'metadata'
        for field_name, field_type in self.metadata_fields:
            if field_name != "metadata":  # Tr√°nh tr√πng l·∫∑p
                defined_field_names = [f.name for f in fields]
                if field_name not in defined_field_names:
                     actual_field_type = field_type
                     if isinstance(field_type, str):
                          try:
                               actual_field_type = getattr(DataType, field_type.upper())
                          except AttributeError:
                               print(f"Warning: Ki·ªÉu d·ªØ li·ªáu metadata kh√¥ng h·ª£p l·ªá '{field_type}' cho tr∆∞·ªùng '{field_name}'. B·ªè qua.")
                               continue

                     if isinstance(actual_field_type, DataType):
                         if actual_field_type == DataType.VARCHAR:
                              fields.append(FieldSchema(name=field_name, dtype=actual_field_type, max_length=1024))  # Gi·∫£m max_length
                         else:
                              fields.append(FieldSchema(name=field_name, dtype=actual_field_type))

        # T·∫°o schema v√† collection
        try:
            schema = CollectionSchema(fields, enable_dynamic_field=False)
            self.collection = Collection(self.collection_name, schema)
            print(f"ƒê√£ t·∫°o collection '{self.collection_name}' v·ªõi schema t·ªëi ∆∞u.")
        except Exception as e:
            print(f"L·ªói khi t·∫°o collection '{self.collection_name}': {e}")
            raise

        # S·ª¨ D·ª§NG METHOD DUY NH·∫§T ƒë·ªÉ t·∫°o index
        self._create_optimized_index("vector")

        # KH√îNG load collection ngay - s·∫Ω load sau khi insert xong
        print(f"Collection '{self.collection_name}' ƒë√£ ƒë∆∞·ª£c t·∫°o. S·∫Ω load sau khi insert d·ªØ li·ªáu.")
    
    def add_texts(
        self, 
        texts: List[str], 
        metadatas: Optional[List[Dict[str, Any]]] = None,
        batch_size: int = 5000,  # TƒÉng batch_size m·∫∑c ƒë·ªãnh
        auto_flush: bool = True,  # Th√™m tham s·ªë auto_flush
        **kwargs
    ) -> List[str]:
        """
        Th√™m vƒÉn b·∫£n v√† metadata v√†o vector store v·ªõi batch processing t·ªëi ∆∞u
        
        Args:
            texts: Danh s√°ch c√°c vƒÉn b·∫£n c·∫ßn th√™m
            metadatas: Danh s√°ch metadata t∆∞∆°ng ·ª©ng v·ªõi m·ªói vƒÉn b·∫£n
            batch_size: K√≠ch th∆∞·ªõc batch ƒë·ªÉ insert (m·∫∑c ƒë·ªãnh 5000)
            auto_flush: C√≥ t·ª± ƒë·ªông flush sau khi insert kh√¥ng (m·∫∑c ƒë·ªãnh True)
            
        Returns:
            List[str]: Danh s√°ch ID c·ªßa c√°c vƒÉn b·∫£n ƒë√£ th√™m
        """
        import time
        start_time = time.time()
        
        print(f"üöÄ B·∫Øt ƒë·∫ßu add_texts: {len(texts):,} records v·ªõi batch_size={batch_size:,}")
        
        # T·∫°o ID n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
        ids = kwargs.get("ids", [str(uuid.uuid4()) for _ in range(len(texts))])
        
        # T·∫°o metadata n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
        if metadatas is None:
            metadatas = [{} for _ in texts]
        
        # ƒê·∫£m b·∫£o collection ƒë√£ ƒë∆∞·ª£c load
        collection_load_start = time.time()
        if not hasattr(self.collection, '_loaded') or not self.collection._loaded:
            try:
                self.collection.load()
                print(f"ƒê√£ load collection '{self.collection_name}' ƒë·ªÉ insert d·ªØ li·ªáu.")
            except Exception as e:
                print(f"Warning: Kh√¥ng th·ªÉ load collection: {e}")
        collection_load_time = time.time() - collection_load_start
        
        all_ids = []
        total_batches = (len(texts) + batch_size - 1) // batch_size
        embedding_time = 0
        insert_time = 0
        
        # Insert theo batch ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t
        for i in range(0, len(texts), batch_size):
            batch_num = i // batch_size + 1
            batch_texts = texts[i:i + batch_size]
            batch_metadatas = metadatas[i:i + batch_size]
            batch_ids = ids[i:i + batch_size]
            
            # Progress info
            progress = (batch_num / total_batches) * 100
            print(f"\nüì¶ BATCH {batch_num}/{total_batches} ({progress:.1f}%)")
            print(f"   üìç Processing: {i+1:,} ‚Üí {min(i+len(batch_texts), len(texts)):,} c·ªßa {len(texts):,}")
            
            # ƒêo th·ªùi gian embedding - WITH BATCH EMBEDDING
            embedding_start = time.time()
            print(f"   üß† Batch embedding {len(batch_texts):,} texts...")
            
            # S·ª¨ D·ª§NG BATCH EMBEDDING - NHANH H∆†N NHI·ªÄU L·∫¶N!
            try:
                # S·ª≠ d·ª•ng helper method ƒë·ªÉ batch embed
                batch_vectors = self._batch_embed_texts(batch_texts, embedding_batch_size=400)
                print(f"   ‚úÖ BATCH EMBEDDING th√†nh c√¥ng ({len(batch_vectors)} vectors)!")
                    
            except Exception as batch_error:
                print(f"   ‚ö†Ô∏è  Batch embedding failed: {batch_error}, fallback to single...")
                # Fallback v·ªÅ single embedding
                batch_vectors = []
                embedding_checkpoint = max(1, len(batch_texts) // 5)
                
                for j, text in enumerate(batch_texts):
                    vector = self.embedding_function(text)
                    batch_vectors.append(vector)
                    
                    if (j + 1) % embedding_checkpoint == 0 or j == len(batch_texts) - 1:
                        embed_progress = ((j + 1) / len(batch_texts)) * 100
                        embed_elapsed = time.time() - embedding_start
                        embed_speed = (j + 1) / embed_elapsed if embed_elapsed > 0 else 0
                        
                        print(f"      üìä {j+1:,}/{len(batch_texts):,} ({embed_progress:.0f}%) | "
                              f"Speed: {embed_speed:.0f}/s")
            
            batch_embedding_time = time.time() - embedding_start
            embedding_time += batch_embedding_time
            embedding_speed = len(batch_texts) / batch_embedding_time if batch_embedding_time > 0 else 0
            
            print(f"   ‚úÖ Embedding: {batch_embedding_time:.2f}s ({embedding_speed:.0f} texts/s)")
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ ch√®n (ch·ªâ c√°c tr∆∞·ªùng c·∫ßn thi·∫øt)
            data_prep_start = time.time()
            print(f"   üîß Chu·∫©n b·ªã insert data...")
            
            # T·∫°o image_ids t·ª´ metadata ho·∫∑c s·ª≠ d·ª•ng batch_ids l√†m fallback
            batch_image_ids = []
            for i, metadata in enumerate(batch_metadatas):
                if isinstance(metadata, dict) and 'image_id' in metadata:
                    batch_image_ids.append(metadata['image_id'])
                elif isinstance(metadata, dict) and 'id' in metadata:
                    batch_image_ids.append(metadata['id'])  # Fallback cho legacy data
                else:
                    batch_image_ids.append(batch_ids[i])  # Fallback cu·ªëi c√πng
            
            data = [
                batch_ids,           # ID field (primary key)
                batch_texts,         # Text field  
                batch_vectors,       # Vector field
                batch_image_ids,     # Image ID field (tr∆∞·ªùng m·ªõi)
                batch_metadatas,     # Metadata field
            ]
            data_prep_time = time.time() - data_prep_start
            print(f"   ‚úÖ Data prep: {data_prep_time:.3f}s")
            
            # ƒêo th·ªùi gian insert - WITH DETAILED LOGGING
            insert_start = time.time()
            print(f"   üíæ Inserting {len(batch_texts):,} records...")
            try:
                insert_result = self.collection.insert(data)
                all_ids.extend(batch_ids)
                batch_insert_time = time.time() - insert_start
                insert_time += batch_insert_time
                
                # Insert performance details
                insert_speed = len(batch_texts) / batch_insert_time if batch_insert_time > 0 else 0
                data_size_mb = (len(batch_texts) * 1000) / (1024 * 1024)  # Rough estimate
                
                print(f"   ‚úÖ Insert: {batch_insert_time:.2f}s ({insert_speed:.0f} rec/s)")
                print(f"      üìä ~{data_size_mb:.1f}MB | IDs: {len(insert_result.primary_keys):,}")
                
                # Batch summary
                batch_total_time = batch_embedding_time + batch_insert_time + data_prep_time
                records_per_sec = len(batch_texts) / batch_total_time if batch_total_time > 0 else 0
                
                print(f"   üéØ BATCH TOTAL: {batch_total_time:.2f}s | Speed: {records_per_sec:.0f} rec/s")
                print(f"      üìà Progress: {len(all_ids):,}/{len(texts):,} completed")
                
                # ETA calculation
                if batch_num > 1:
                    avg_batch_time = (embedding_time + insert_time) / batch_num
                    remaining_batches = total_batches - batch_num
                    eta_seconds = remaining_batches * avg_batch_time
                    eta_minutes = eta_seconds / 60
                    
                    print(f"      üïê ETA: {eta_minutes:.1f} ph√∫t")
                    
                    # Mini progress bar
                    bar_length = 20
                    filled = int(bar_length * progress / 100)
                    bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)
                    print(f"      üìä [{bar}] {progress:.1f}%")
                
            except Exception as e:
                print(f"   ‚ùå L·ªói insert batch {batch_num}: {e}")
                raise
        
        # ƒêo th·ªùi gian flush
        flush_start = time.time()
        if auto_flush:
            try:
                self.collection.flush()
                flush_time = time.time() - flush_start
                print(f"‚úÖ ƒê√£ flush {len(all_ids):,} records trong {flush_time:.2f}s")
            except Exception as e:
                print(f"Warning: L·ªói khi flush: {e}")
        else:
            flush_time = 0
        
        # T√≠nh t·ªïng th·ªùi gian v√† th·ªëng k√™
        total_time = time.time() - start_time
        overall_speed = len(all_ids) / total_time
        
        print(f"üéâ Ho√†n th√†nh add_texts!")
        print(f"üìä TH·ªêNG K√ä HI·ªÜU SU·∫§T:")
        print(f"   üìù Records: {len(all_ids):,}/{len(texts):,}")
        print(f"   ‚è∞ T·ªïng th·ªùi gian: {total_time:.2f}s")
        print(f"   üîÑ Collection load: {collection_load_time:.2f}s")
        print(f"   üß† Embedding: {embedding_time:.2f}s ({embedding_time/total_time*100:.1f}%)")
        print(f"   üíæ Insert: {insert_time:.2f}s ({insert_time/total_time*100:.1f}%)")
        print(f"   üöÄ Flush: {flush_time:.2f}s ({flush_time/total_time*100:.1f}%)")
        print(f"   ‚ö° T·ªëc ƒë·ªô: {overall_speed:.0f} records/second")
        
        return all_ids
    
    def similarity_search(
        self, 
        query: str, 
        k: int = 4, 
        **kwargs
    ) -> List[Tuple[Any, float]]:
        """
        T√¨m ki·∫øm vƒÉn b·∫£n t∆∞∆°ng t·ª± v·ªõi query d·ª±a tr√™n ƒë·ªô t∆∞∆°ng ƒë·ªìng v·ªÅ vector
        
        Args:
            query: C√¢u truy v·∫•n
            k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ
            
        Returns:
            List[Tuple[Any, float]]: Danh s√°ch tuple g·ªìm t√†i li·ªáu v√† ƒë·ªô t∆∞∆°ng ƒë·ªìng
        """
        # T·∫°o vector t·ª´ query
        query_vector = self.embedding_function(query)
        
        # Th·ª±c hi·ªán t√¨m ki·∫øm
        search_params = {"metric_type": "COSINE", "params": {"ef": 64}}
        
        expr = None
        
        results = self.collection.search(
            data=[query_vector],
            anns_field=self.text_vector_field,
            param=search_params,
            limit=k,
            expr=expr,
            output_fields=[self.text_field, "metadata"]
        )
        
        # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ sang ƒë·ªãnh d·∫°ng tr·∫£ v·ªÅ
        docs_with_scores = []
        for hit in results[0]:
            doc = {
                "id": hit.id,
                "text": hit.entity.get(self.text_field),
                "metadata": hit.entity.get("metadata")
            }
            docs_with_scores.append((doc, hit.score))
        
        return docs_with_scores
    
    def fulltext_search(
        self, 
        query: str, 
        k: int = 4, 
        **kwargs
    ) -> List[Tuple[Any, float]]:
        """
        T√¨m ki·∫øm vƒÉn b·∫£n d·ª±a tr√™n full-text search
        
        Args:
            query: C√¢u truy v·∫•n
            k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ
            
        Returns:
            List[Tuple[Any, float]]: Danh s√°ch tuple g·ªìm t√†i li·ªáu v√† ƒëi·ªÉm s·ªë
        """
        try:
            # Th·ª±c hi·ªán full-text search
            expr = f'{self.text_field} like "%{query}%"'
            
            results = self.collection.query(
                expr=expr,
                output_fields=[self.id_field, self.text_field, "metadata"],
                limit=k
            )
            
            # T√≠nh to√°n ƒëi·ªÉm s·ªë ƒë∆°n gi·∫£n d·ª±a tr√™n s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´ kh√≥a
            docs_with_scores = []
            query_words = query.lower().split()
            
            for item in results:
                text = item.get(self.text_field, "").lower()
                score = 0
                for word in query_words:
                    score += text.count(word)
                
                # Chu·∫©n h√≥a ƒëi·ªÉm s·ªë
                if len(query_words) > 0:
                    score = score / len(query_words)
                
                doc = {
                    "id": item.get(self.id_field),
                    "text": item.get(self.text_field),
                    "metadata": item.get("metadata", {})
                }
                docs_with_scores.append((doc, min(score, 1.0)))
            
            # S·∫Øp x·∫øp k·∫øt qu·∫£ theo ƒëi·ªÉm s·ªë gi·∫£m d·∫ßn
            docs_with_scores.sort(key=lambda x: x[1], reverse=True)
            return docs_with_scores[:k]
            
        except Exception as e:
            print(f"L·ªói khi th·ª±c hi·ªán fulltext search: {e}")
            return []
    
    def hybrid_search(
        self, 
        query: str, 
        k: int = 4, 
        alpha: float = 0.5, 
        **kwargs
    ) -> List[Tuple[Any, float]]:
        """
        T√¨m ki·∫øm vƒÉn b·∫£n k·∫øt h·ª£p gi·ªØa vector similarity v√† full-text search
        
        Args:
            query: C√¢u truy v·∫•n
            k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ
            alpha: Tr·ªçng s·ªë cho k·∫øt qu·∫£ vector similarity (0-1)
            
        Returns:
            List[Tuple[Any, float]]: Danh s√°ch tuple g·ªìm t√†i li·ªáu v√† ƒëi·ªÉm s·ªë k·∫øt h·ª£p
        """
        # L·∫•y k·∫øt qu·∫£ t·ª´ c·∫£ hai ph∆∞∆°ng ph√°p
        vector_results = self.similarity_search(query, k=k*2, **kwargs)
        fulltext_results = self.fulltext_search(query, k=k*2, **kwargs)
        
        # T·∫°o map t·ª´ ID ƒë·∫øn k·∫øt qu·∫£ v√† ƒëi·ªÉm s·ªë
        results_map = {}
        
        # Th√™m k·∫øt qu·∫£ vector search v·ªõi tr·ªçng s·ªë alpha
        for doc, score in vector_results:
            doc_id = doc["id"]
            if doc_id not in results_map:
                results_map[doc_id] = {"doc": doc, "vector_score": 0, "text_score": 0}
            results_map[doc_id]["vector_score"] = score
        
        # Th√™m k·∫øt qu·∫£ fulltext search v·ªõi tr·ªçng s·ªë (1-alpha)
        for doc, score in fulltext_results:
            doc_id = doc["id"]
            if doc_id not in results_map:
                results_map[doc_id] = {"doc": doc, "vector_score": 0, "text_score": 0}
            results_map[doc_id]["text_score"] = score
        
        # T√≠nh ƒëi·ªÉm k·∫øt h·ª£p
        hybrid_results = []
        for doc_id, result in results_map.items():
            hybrid_score = alpha * result["vector_score"] + (1 - alpha) * result["text_score"]
            hybrid_results.append((result["doc"], hybrid_score))
        
        # S·∫Øp x·∫øp theo ƒëi·ªÉm k·∫øt h·ª£p gi·∫£m d·∫ßn
        hybrid_results.sort(key=lambda x: x[1], reverse=True)
        
        return hybrid_results[:k]
    
    def delete(self, ids: List[str], **kwargs) -> None:
        """
        X√≥a c√°c vƒÉn b·∫£n kh·ªèi vector store d·ª±a tr√™n ID
        
        Args:
            ids: Danh s√°ch ID c·∫ßn x√≥a
        """
        expr = f"{self.id_field} in {ids}"
        self.collection.delete(expr)
        self.collection.flush()
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """
        L·∫•y th√¥ng tin th·ªëng k√™ v·ªÅ collection
        
        Returns:
            Dict[str, Any]: Th√¥ng tin th·ªëng k√™
        """
        stats = {
            "name": self.collection_name,
            "count": self.collection.num_entities,
            "fields": self.collection.schema.fields,
        }
        
        return stats

    def optimize_collection(self) -> None:
        """
        T·ªëi ∆∞u h√≥a collection sau khi insert d·ªØ li·ªáu xong
        - Compact ƒë·ªÉ gi·∫£m ph√¢n m·∫£nh
        - Rebuild index n·∫øu c·∫ßn
        """
        try:
            print(f"ƒêang t·ªëi ∆∞u h√≥a collection '{self.collection_name}'...")
            
            # Flush ƒë·ªÉ ƒë·∫£m b·∫£o t·∫•t c·∫£ d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c ghi
            self.collection.flush()
            
            # Compact ƒë·ªÉ gi·∫£m ph√¢n m·∫£nh
            self.collection.compact()
            print("ƒê√£ th·ª±c hi·ªán compact collection.")
            
            # Ki·ªÉm tra v√† rebuild index n·∫øu c·∫ßn
            index_info = self.collection.indexes
            if index_info:
                print("Index ƒë√£ t·ªìn t·∫°i, kh√¥ng c·∫ßn rebuild.")
            else:
                print("ƒêang rebuild index...")
                # S·ª¨ D·ª§NG METHOD DUY NH·∫§T ƒë·ªÉ t·∫°o index
                self._create_optimized_index("vector")
            
            # Load collection ƒë·ªÉ s·ª≠ d·ª•ng
            self.collection.load()
            print(f"Collection '{self.collection_name}' ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a v√† load.")
            
        except Exception as e:
            print(f"L·ªói khi t·ªëi ∆∞u h√≥a collection: {e}")

    def bulk_insert_texts(
        self, 
        texts: List[str], 
        metadatas: Optional[List[Dict[str, Any]]] = None,
        batch_size: int = 10000,  # TƒÉng batch size cho bulk insert
        **kwargs
    ) -> List[str]:
        """
        Insert h√†ng lo·∫°t v·ªõi t·ªëi ∆∞u h√≥a cao nh·∫•t
        
        Args:
            texts: Danh s√°ch vƒÉn b·∫£n
            metadatas: Danh s√°ch metadata
            batch_size: K√≠ch th∆∞·ªõc batch (m·∫∑c ƒë·ªãnh 10000)
            
        Returns:
            List[str]: Danh s√°ch ID ƒë√£ insert
        """
        import time
        start_time = time.time()
        
        print(f"üöÄ B·∫Øt ƒë·∫ßu BULK INSERT: {len(texts):,} records v·ªõi batch_size={batch_size:,}")
        
        # Insert t·∫•t c·∫£ batch m√† kh√¥ng flush
        insert_start = time.time()
        all_ids = self.add_texts(
            texts=texts, 
            metadatas=metadatas, 
            batch_size=batch_size,
            auto_flush=False,  # Kh√¥ng flush t·ª´ng batch
            **kwargs
        )
        insert_time = time.time() - insert_start
        
        # Flush m·ªôt l·∫ßn duy nh·∫•t ·ªü cu·ªëi
        flush_start = time.time()
        print("üîÑ ƒêang flush t·∫•t c·∫£ d·ªØ li·ªáu...")
        self.collection.flush()
        flush_time = time.time() - flush_start
        
        # T·ªëi ∆∞u h√≥a collection
        optimize_start = time.time()
        print("‚ö° ƒêang t·ªëi ∆∞u h√≥a collection...")
        self.optimize_collection()
        optimize_time = time.time() - optimize_start
        
        # T√≠nh t·ªïng th·ªùi gian
        total_time = time.time() - start_time
        overall_speed = len(all_ids) / total_time
        
        print(f"üéâ HO√ÄN TH√ÄNH BULK INSERT!")
        print(f"üìä TH·ªêNG K√ä T·ªîNG QUAN:")
        print(f"   üìù Records: {len(all_ids):,}")
        print(f"   ‚è∞ T·ªïng th·ªùi gian: {total_time:.2f}s")
        print(f"   üíæ Insert + Embedding: {insert_time:.2f}s ({insert_time/total_time*100:.1f}%)")
        print(f"   üöÄ Final Flush: {flush_time:.2f}s ({flush_time/total_time*100:.1f}%)")
        print(f"   ‚öôÔ∏è  Optimization: {optimize_time:.2f}s ({optimize_time/total_time*100:.1f}%)")
        print(f"   ‚ö° T·ªëc ƒë·ªô t·ªïng: {overall_speed:.0f} records/second")
        
        return all_ids

    def mega_insert_texts(
        self, 
        texts: List[str], 
        metadatas: Optional[List[Dict[str, Any]]] = None,
        batch_size: int = 50000,  # Batch r·∫•t l·ªõn cho mega insert
        **kwargs
    ) -> List[str]:
        """
        Insert si√™u l·ªõn cho h√†ng tri·ªáu b·∫£n ghi v·ªõi t·ªëi ∆∞u h√≥a t·ªëi ƒëa
        
        Args:
            texts: Danh s√°ch vƒÉn b·∫£n
            metadatas: Danh s√°ch metadata  
            batch_size: K√≠ch th∆∞·ªõc batch (m·∫∑c ƒë·ªãnh 50,000)
            
        Returns:
            List[str]: Danh s√°ch ID ƒë√£ insert
        """
        import time
        start_time = time.time()
        
        total_records = len(texts)
        print(f"üöÄ B·∫Øt ƒë·∫ßu MEGA INSERT {total_records:,} records v·ªõi batch_size={batch_size:,}")
        
        if total_records > 1000000:  # > 1 tri·ªáu
            print("‚ö†Ô∏è  C·∫¢NH B√ÅO: Insert h∆°n 1 tri·ªáu records. ƒê·∫£m b·∫£o ƒë·ªß RAM v√† th·ªùi gian!")
        
        # T·∫°o ID n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
        preparation_start = time.time()
        ids = kwargs.get("ids", [str(uuid.uuid4()) for _ in range(total_records)])
        
        # T·∫°o metadata n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p
        if metadatas is None:
            metadatas = [{} for _ in texts]
        preparation_time = time.time() - preparation_start
        
        # ƒê·∫£m b·∫£o collection ƒë√£ ƒë∆∞·ª£c load
        load_start = time.time()
        if not hasattr(self.collection, '_loaded') or not self.collection._loaded:
            try:
                self.collection.load()
                print(f"ƒê√£ load collection '{self.collection_name}' ƒë·ªÉ insert d·ªØ li·ªáu.")
            except Exception as e:
                print(f"Warning: Kh√¥ng th·ªÉ load collection: {e}")
        load_time = time.time() - load_start
        
        all_ids = []
        total_batches = (total_records + batch_size - 1) // batch_size
        total_embedding_time = 0
        total_insert_time = 0
        
        # Insert theo batch si√™u l·ªõn
        for i in range(0, total_records, batch_size):
            batch_num = i // batch_size + 1
            batch_texts = texts[i:i + batch_size]
            batch_metadatas = metadatas[i:i + batch_size]
            batch_ids = ids[i:i + batch_size]
            
            batch_start = time.time()
            progress = (batch_num / total_batches) * 100
            print(f"\nüì¶ BATCH {batch_num}/{total_batches} ({progress:.1f}%) - {len(batch_texts):,} records")
            print(f"   üìç Records: {i+1:,} ‚Üí {min(i+len(batch_texts), total_records):,}")
            
            # T·∫°o vectors t·ª´ batch texts - WITH BATCH EMBEDDING
            embedding_start = time.time()
            print(f"   üß† Batch embedding {len(batch_texts):,} texts...")
            
            # S·ª¨ D·ª§NG BATCH EMBEDDING - SI√äU NHANH!
            try:
                # S·ª≠ d·ª•ng helper method ƒë·ªÉ batch embed
                batch_vectors = self._batch_embed_texts(batch_texts, embedding_batch_size=400)
                print(f"   üöÄ BATCH EMBEDDING th√†nh c√¥ng ({len(batch_vectors)} vectors) - si√™u t·ªëi ∆∞u!")
                    
            except Exception as batch_error:
                print(f"   ‚ö†Ô∏è  Batch embedding error: {batch_error}, fallback...")
                # Fallback to single embedding v·ªõi progress tracking
                batch_vectors = []
                embedding_checkpoint = max(1, len(batch_texts) // 4)
                
                for j, text in enumerate(batch_texts):
                    vector = self.embedding_function(text)
                    batch_vectors.append(vector)
                    
                    if (j + 1) % embedding_checkpoint == 0 or j == len(batch_texts) - 1:
                        embed_progress = ((j + 1) / len(batch_texts)) * 100
                        embed_elapsed = time.time() - embedding_start
                        embed_speed = (j + 1) / embed_elapsed
                        remaining_embeds = len(batch_texts) - (j + 1)
                        embed_eta = remaining_embeds / embed_speed if embed_speed > 0 else 0
                        
                        print(f"      üìä Embedding: {j+1:,}/{len(batch_texts):,} ({embed_progress:.0f}%) | "
                              f"Speed: {embed_speed:.0f}/s | ETA: {embed_eta:.1f}s")
            
            embedding_time = time.time() - embedding_start
            total_embedding_time += embedding_time
            embedding_speed = len(batch_texts) / embedding_time
            
            print(f"   ‚úÖ Embedding ho√†n th√†nh: {embedding_time:.1f}s ({embedding_speed:.0f} texts/s)")
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ ch√®n
            data_prep_start = time.time()
            print(f"   üîß Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ insert...")
            
            # T·∫°o image_ids t·ª´ metadata ho·∫∑c s·ª≠ d·ª•ng batch_ids l√†m fallback
            batch_image_ids = []
            for j, metadata in enumerate(batch_metadatas):
                if isinstance(metadata, dict) and 'image_id' in metadata:
                    batch_image_ids.append(metadata['image_id'])
                elif isinstance(metadata, dict) and 'id' in metadata:
                    batch_image_ids.append(metadata['id'])  # Fallback cho legacy data
                else:
                    batch_image_ids.append(batch_ids[j])  # Fallback cu·ªëi c√πng
            
            data = [
                batch_ids,           # ID field (primary key - UUID)
                batch_texts,         # Text field
                batch_vectors,       # Vector field
                batch_image_ids,     # Image ID field (tr∆∞·ªùng m·ªõi)
                batch_metadatas,     # Metadata field
            ]
            data_prep_time = time.time() - data_prep_start
            print(f"   ‚úÖ D·ªØ li·ªáu ƒë√£ chu·∫©n b·ªã: {data_prep_time:.2f}s")
            
            # Ch√®n batch v√†o collection - WITH DETAILED LOGGING  
            insert_start = time.time()
            print(f"   üíæ B·∫Øt ƒë·∫ßu insert {len(batch_texts):,} records v√†o Milvus...")
            
            try:
                insert_result = self.collection.insert(data)
                all_ids.extend(batch_ids)
                insert_time = time.time() - insert_start
                total_insert_time += insert_time
                
                # Chi ti·∫øt v·ªÅ insert performance
                insert_speed = len(batch_texts) / insert_time
                data_size_mb = (len(batch_texts) * (1000 + 512 * 4)) / (1024 * 1024)  # ∆Ø·ªõc t√≠nh MB
                
                print(f"   ‚úÖ Insert ho√†n th√†nh: {insert_time:.1f}s ({insert_speed:.0f} rec/s)")
                print(f"      üìä Data size: ~{data_size_mb:.1f}MB | Insert IDs: {len(insert_result.primary_keys):,}")
                
                # Overall batch performance
                batch_total_time = time.time() - batch_start
                batch_speed = len(batch_texts) / batch_total_time
                
                print(f"   üéØ BATCH SUMMARY:")
                print(f"      ‚è±Ô∏è  Total: {batch_total_time:.1f}s | Embedding: {embedding_time:.1f}s ({embedding_time/batch_total_time*100:.0f}%) | Insert: {insert_time:.1f}s ({insert_time/batch_total_time*100:.0f}%)")
                print(f"      ‚ö° Speed: {batch_speed:.0f} rec/s total | {len(all_ids):,}/{total_records:,} completed")
                
                # ∆Ø·ªõc t√≠nh th·ªùi gian c√≤n l·∫°i - IMPROVED ETA
                if batch_num > 1:
                    avg_time_per_batch = (time.time() - start_time - preparation_time - load_time) / batch_num
                    remaining_batches = total_batches - batch_num
                    eta_seconds = remaining_batches * avg_time_per_batch
                    eta_minutes = eta_seconds / 60
                    eta_hours = eta_minutes / 60
                    
                    if eta_hours >= 1:
                        print(f"      üïê ETA: {eta_hours:.1f} gi·ªù ({eta_minutes:.0f} ph√∫t)")
                    else:
                        print(f"      üïê ETA: {eta_minutes:.1f} ph√∫t")
                    
                    # Progress bar visual
                    bar_length = 30
                    filled_length = int(bar_length * progress / 100)
                    bar = '‚ñà' * filled_length + '‚ñë' * (bar_length - filled_length)
                    print(f"      üìà [{bar}] {progress:.1f}%")
                
            except Exception as e:
                print(f"   ‚ùå L·ªói khi insert batch {batch_num}: {e}")
                raise
        
        # Flush m·ªôt l·∫ßn duy nh·∫•t ·ªü cu·ªëi
        flush_start = time.time()
        print("üîÑ ƒêang flush t·∫•t c·∫£ d·ªØ li·ªáu...")
        self.collection.flush()
        flush_time = time.time() - flush_start
        
        # T·ªëi ∆∞u h√≥a collection
        optimize_start = time.time()
        print("‚ö° ƒêang t·ªëi ∆∞u h√≥a collection...")
        self.optimize_collection()
        optimize_time = time.time() - optimize_start
        
        # T√≠nh to√°n th·ªëng k√™ t·ªïng quan
        total_time = time.time() - start_time
        overall_speed = len(all_ids) / total_time
        
        print(f"üéâ HO√ÄN TH√ÄNH MEGA INSERT {len(all_ids):,} records!")
        print(f"üìä TH·ªêNG K√ä MEGA INSERT:")
        print(f"   üìù Records: {len(all_ids):,}")
        print(f"   ‚è∞ T·ªïng th·ªùi gian: {total_time/60:.1f} ph√∫t ({total_time:.1f}s)")
        print(f"   üîß Preparation: {preparation_time:.1f}s ({preparation_time/total_time*100:.1f}%)")
        print(f"   üîÑ Collection load: {load_time:.1f}s ({load_time/total_time*100:.1f}%)")
        print(f"   üß† Total Embedding: {total_embedding_time:.1f}s ({total_embedding_time/total_time*100:.1f}%)")
        print(f"   üíæ Total Insert: {total_insert_time:.1f}s ({total_insert_time/total_time*100:.1f}%)")
        print(f"   üöÄ Final Flush: {flush_time:.1f}s ({flush_time/total_time*100:.1f}%)")
        print(f"   ‚öôÔ∏è  Optimization: {optimize_time:.1f}s ({optimize_time/total_time*100:.1f}%)")
        print(f"   ‚ö° T·ªëc ƒë·ªô trung b√¨nh: {overall_speed:.0f} records/second")
        print(f"   üí∞ Chi ph√≠ th·ªùi gian tr√™n 1K records: {total_time/total_records*1000:.2f}s")
        
        return all_ids

    def upsert_image(
        self,
        image_id: str,
        image_name: str,
        image_path: str,
        category: str,
        style: str = "",
        app_name: str = "",
        **kwargs
    ) -> Dict[str, Any]:
        """
        Upsert (insert or update) m·ªôt image record trong Milvus
        
        Args:
            image_id: ID c·ªßa image (primary key)
            image_name: T√™n c·ªßa image (s·∫Ω ƒë∆∞·ª£c embedding)
            image_path: ƒê∆∞·ªùng d·∫´n file image
            category: Danh m·ª•c
            style: Style c·ªßa image
            app_name: T√™n app
            
        Returns:
            Dict[str, Any]: K·∫øt qu·∫£ upsert v·ªõi th√¥ng tin chi ti·∫øt
        """
        import time
        start_time = time.time()
        
        print(f"üîÑ Upsert image: {image_id} - {image_name[:50]}...")
        
        try:
            # ƒê·∫£m b·∫£o collection ƒë√£ ƒë∆∞·ª£c load
            if not hasattr(self.collection, '_loaded') or not self.collection._loaded:
                self.collection.load()
            
            # Ki·ªÉm tra xem record ƒë√£ t·ªìn t·∫°i ch∆∞a (theo image_id field)
            check_start = time.time()
            existing_expr = f"image_id == '{image_id}'"
            existing_results = self.collection.query(
                expr=existing_expr,
                output_fields=[self.id_field, self.text_field, "image_id", "metadata"],
                limit=1
            )
            check_time = time.time() - check_start
            
            is_update = len(existing_results) > 0
            action_type = "UPDATE" if is_update else "INSERT"
            
            print(f"   üîç Check existence: {check_time:.3f}s - {action_type}")
            
            # Merge d·ªØ li·ªáu c≈© v·ªõi d·ªØ li·ªáu m·ªõi
            merge_start = time.time()
            if is_update:
                # L·∫•y d·ªØ li·ªáu c≈©
                old_record = existing_results[0]
                old_metadata = old_record.get("metadata", {})
                old_text = old_record.get(self.text_field, "")
                old_primary_key = old_record.get(self.id_field, "")  # Gi·ªØ nguy√™n primary key c≈©
                
                print(f"   üìã Merging with existing data...")
                print(f"      Primary Key: {old_primary_key} (preserved)")
                print(f"      Old: {old_text[:30]}...")
                print(f"      New: {image_name[:30]}...")
                
                # Merge metadata: Gi·ªØ d·ªØ li·ªáu c≈©, override v·ªõi d·ªØ li·ªáu m·ªõi
                merged_metadata = old_metadata.copy()
                merged_metadata.update({
                    "image_id": image_id,
                    "image_path": image_path,
                    "image_name": image_name,
                    "category": category,
                    "style": style,
                    "app_name": app_name
                })
                
                # S·ª≠ d·ª•ng d·ªØ li·ªáu merged v·ªõi primary key c≈©
                final_primary_key = old_primary_key  # Gi·ªØ nguy√™n UUID c≈©
                final_image_name = image_name
                final_metadata = merged_metadata
                
                # X√≥a record c≈© sau khi ƒë√£ l·∫•y d·ªØ li·ªáu
                delete_start = time.time()
                print(f"   üóëÔ∏è  Deleting old record...")
                self.collection.delete(existing_expr)
                delete_time = time.time() - delete_start
                print(f"   ‚úÖ Delete completed: {delete_time:.3f}s")
            else:
                # D·ªØ li·ªáu ho√†n to√†n m·ªõi - t·∫°o UUID m·ªõi
                import uuid
                final_primary_key = str(uuid.uuid4())  # UUID m·ªõi cho INSERT
                final_image_name = image_name
                final_metadata = {
                    "image_id": image_id,
                    "image_path": image_path,
                    "image_name": image_name,
                    "category": category,
                    "style": style,
                    "app_name": app_name
                }
                delete_time = 0
                print(f"   üÜï New record with UUID: {final_primary_key}")
            
            merge_time = time.time() - merge_start
            print(f"   üîÄ Data merge: {merge_time:.3f}s")
            
            # T·∫°o embedding cho final_image_name
            embedding_start = time.time()
            print(f"   üß† Creating embedding for: {final_image_name}")
            image_vector = self.embedding_function(final_image_name)
            embedding_time = time.time() - embedding_start
            print(f"   ‚úÖ Embedding: {embedding_time:.3f}s ({len(image_vector)} dims)")
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ insert v·ªõi primary key ƒë√∫ng
            data_prep_start = time.time()
            data = [
                [final_primary_key],  # ID field (UUID c≈© n·∫øu UPDATE, UUID m·ªõi n·∫øu INSERT)
                [final_image_name],   # Text field (ƒë√£ merge)
                [image_vector],       # Vector field
                [image_id],           # Image ID field (business ID)
                [final_metadata]      # Metadata field (ƒë√£ merge)
            ]
            data_prep_time = time.time() - data_prep_start
            
            # Insert record m·ªõi
            insert_start = time.time()
            print(f"   üíæ Inserting new record...")
            insert_result = self.collection.insert(data)
            insert_time = time.time() - insert_start
            print(f"   ‚úÖ Insert: {insert_time:.3f}s")
            
            # Flush ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c ghi
            flush_start = time.time()
            self.collection.flush()
            flush_time = time.time() - flush_start
            print(f"   üöÄ Flush: {flush_time:.3f}s")
            
            # T√≠nh to√°n t·ªïng th·ªùi gian
            total_time = time.time() - start_time
            
            print(f"üéâ {action_type} completed!")
            print(f"   ‚è∞ Total time: {total_time:.3f}s")
            print(f"   üìä Check: {check_time:.3f}s | Merge: {merge_time:.3f}s | Embedding: {embedding_time:.3f}s | Insert: {insert_time:.3f}s | Flush: {flush_time:.3f}s")
            
            return {
                "success": True,
                "action": action_type.lower(),
                "primary_key": final_primary_key,  # UUID th·ª±c t·∫ø trong DB
                "image_id": image_id,              # Business ID
                "image_name": final_image_name,    # S·ª≠ d·ª•ng final name
                "is_update": is_update,
                "timing": {
                    "total_time": total_time,
                    "check_time": check_time,
                    "merge_time": merge_time,
                    "delete_time": delete_time if is_update else 0,
                    "embedding_time": embedding_time,
                    "insert_time": insert_time,
                    "flush_time": flush_time
                },
                "metadata": final_metadata,
                "insert_result": {
                    "primary_keys": len(insert_result.primary_keys),
                    "insert_count": insert_result.insert_count
                }
            }
            
        except Exception as e:
            total_time = time.time() - start_time
            error_msg = f"‚ùå Upsert failed for {image_id}: {str(e)}"
            print(error_msg)
            import traceback
            traceback.print_exc()
            
            return {
                "success": False,
                "action": "error",
                "image_id": image_id,
                "image_name": image_name,
                "error": str(e),
                "timing": {
                    "total_time": total_time
                }
            }

    def get_image_by_id(self, image_id: str) -> Dict[str, Any]:
        """
        L·∫•y th√¥ng tin image theo image_id (kh√¥ng ph·∫£i primary key)
        
        Args:
            image_id: ID c·ªßa image trong tr∆∞·ªùng image_id
            
        Returns:
            Dict[str, Any]: Th√¥ng tin image ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
        """
        try:
            # ƒê·∫£m b·∫£o collection ƒë√£ ƒë∆∞·ª£c load
            if not hasattr(self.collection, '_loaded') or not self.collection._loaded:
                self.collection.load()
            
            # Query ƒë·ªÉ l·∫•y image theo image_id field (kh√¥ng ph·∫£i primary key)
            expr = f"image_id == '{image_id}'"
            results = self.collection.query(
                expr=expr,
                output_fields=[self.id_field, self.text_field, "image_id", "metadata"],
                limit=1
            )
            
            if results:
                result = results[0]
                metadata = result.get("metadata", {})
                
                return {
                    "success": True,
                    "found": True,
                    "data": {
                        "id": result.get(self.id_field),  # Primary key UUID
                        "image_id": result.get("image_id", ""),  # Tr∆∞·ªùng image_id 
                        "text": result.get(self.text_field),
                        "image_path": metadata.get("image_path", ""),
                        "image_name": metadata.get("image_name", ""),
                        "category": metadata.get("category", ""),
                        "style": metadata.get("style", ""),
                        "app_name": metadata.get("app_name", ""),
                        "metadata": metadata
                    }
                }
            else:
                return {
                    "success": True,
                    "found": False,
                    "message": f"Image v·ªõi image_id '{image_id}' kh√¥ng t·ªìn t·∫°i"
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "image_id": image_id
            }